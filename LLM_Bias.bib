
@article{lippens_state_2023,
	title = {The state of hiring discrimination: {A} meta-analysis of (almost) all recent correspondence experiments},
	volume = {151},
	issn = {00142921},
	shorttitle = {The state of hiring discrimination},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0014292122001957},
	doi = {10.1016/j.euroecorev.2022.104315},
	language = {en},
	urldate = {2023-09-20},
	journal = {European Economic Review},
	author = {Lippens, Louis and Vermeiren, Siel and Baert, Stijn},
	month = jan,
	year = {2023},
	pages = {104315},
	file = {Full Text:/Users/daniel/Zotero/storage/6SYAQM7R/Lippens et al. - 2023 - The state of hiring discrimination A meta-analysi.pdf:application/pdf},
}

@article{zschirnt_ethnic_2016,
	title = {Ethnic discrimination in hiring decisions: a meta-analysis of correspondence tests 1990–2015},
	volume = {42},
	issn = {1369-183X, 1469-9451},
	shorttitle = {Ethnic discrimination in hiring decisions},
	url = {http://www.tandfonline.com/doi/full/10.1080/1369183X.2015.1133279},
	doi = {10.1080/1369183X.2015.1133279},
	language = {en},
	number = {7},
	urldate = {2023-09-20},
	journal = {Journal of Ethnic and Migration Studies},
	author = {Zschirnt, Eva and Ruedin, Didier},
	month = may,
	year = {2016},
	pages = {1115--1134},
	file = {Submitted Version:/Users/daniel/Zotero/storage/U5GKSAUP/Zschirnt and Ruedin - 2016 - Ethnic discrimination in hiring decisions a meta-.pdf:application/pdf},
}

@article{chen_ethics_2023,
	title = {Ethics and discrimination in artificial intelligence-enabled recruitment practices},
	volume = {10},
	issn = {2662-9992},
	url = {https://www.nature.com/articles/s41599-023-02079-x},
	doi = {10.1057/s41599-023-02079-x},
	abstract = {Abstract
            This study aims to address the research gap on algorithmic discrimination caused by AI-enabled recruitment and explore technical and managerial solutions. The primary research approach used is a literature review. The findings suggest that AI-enabled recruitment has the potential to enhance recruitment quality, increase efficiency, and reduce transactional work. However, algorithmic bias results in discriminatory hiring practices based on gender, race, color, and personality traits. The study indicates that algorithmic bias stems from limited raw data sets and biased algorithm designers. To mitigate this issue, it is recommended to implement technical measures, such as unbiased dataset frameworks and improved algorithmic transparency, as well as management measures like internal corporate ethical governance and external oversight. Employing Grounded Theory, the study conducted survey analysis to collect firsthand data on respondents’ experiences and perceptions of AI-driven recruitment applications and discrimination.},
	language = {en},
	number = {1},
	urldate = {2023-11-10},
	journal = {Humanities and Social Sciences Communications},
	author = {Chen, Zhisheng},
	month = sep,
	year = {2023},
	pages = {567},
	file = {Full Text:/Users/daniel/Zotero/storage/7DZB4KSN/Chen - 2023 - Ethics and discrimination in artificial intelligen.pdf:application/pdf},
}

@article{budhwar_human_2023,
	title = {Human resource management in the age of generative artificial intelligence: {Perspectives} and research directions on {ChatGPT}},
	volume = {33},
	issn = {0954-5395, 1748-8583},
	shorttitle = {Human resource management in the age of generative artificial intelligence},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1748-8583.12524},
	doi = {10.1111/1748-8583.12524},
	abstract = {Abstract
            ChatGPT and its variants that use generative artificial intelligence (AI) models have rapidly become a focal point in academic and media discussions about their potential benefits and drawbacks across various sectors of the economy, democracy, society, and environment. It remains unclear whether these technologies result in job displacement or creation, or if they merely shift human labour by generating new, potentially trivial or practically irrelevant, information and decisions. According to the CEO of ChatGPT, the potential impact of this new family of AI technology could be as big as “the printing press”, with significant implications for employment, stakeholder relationships, business models, and academic research, and its full consequences are largely undiscovered and uncertain. The introduction of more advanced and potent generative AI tools in the AI market, following the launch of ChatGPT, has ramped up the “AI arms race”, creating continuing uncertainty for workers, expanding their business applications, while heightening risks related to well‐being, bias, misinformation, context insensitivity, privacy issues, ethical dilemmas, and security. Given these developments, this perspectives editorial offers a collection of perspectives and research pathways to extend HRM scholarship in the realm of generative AI. In doing so, the discussion synthesizes the literature on AI and generative AI, connecting it to various aspects of HRM processes, practices, relationships, and outcomes, thereby contributing to shaping the future of HRM research.
          , 
            Key points
            
              What is currently known?
              
                
                  The rapid evolution of artificial intelligence models has swiftly prompted much academic and media discourse regarding their potential for disruption as well as their transformative power impacting multiple facets of the economy, society, and environment.
                
                
                  Software tools like ChatGPT and other comparable ones utilizing generative AI models can produce incredibly human‐like responses to queries, yet, they can also be profoundly erroneous, raising significant ethical and moral issues, and their adoption by HRM practitioners.
                
              
            
            
              What this perspectives editorial adds?
              
                
                  Provides a comprehensive summary of the advancements, constraints, and commercial applications of generative AI.
                
                
                  Offers 11 perspectives that advance scholarship in HRM and present a collection of unexplored research opportunities for HRM scholars.
                
              
            
            
              The implications for practitioners
              
                
                  Comprehending the possible strengths and weaknesses of implementing immersive technologies like ChatGPT and its variants in HRM strategy, practices, procedures, platforms, and productivity will aid organisations' leaders in critically evaluating its relevance, feasibility to implement, usefulness and potential impact to achieve organisationally valued outcomes.
                
                
                  The lack of regulations heightens the risks and ethical dilemmas associated with the usage of generative AI models, which presents significant threats for organisations, scholarly research, and society at large.},
	language = {en},
	number = {3},
	urldate = {2023-11-10},
	journal = {Human Resource Management Journal},
	author = {Budhwar, Pawan and Chowdhury, Soumyadeb and Wood, Geoffrey and Aguinis, Herman and Bamber, Greg J. and Beltran, Jose R. and Boselie, Paul and Lee Cooke, Fang and Decker, Stephanie and DeNisi, Angelo and Dey, Prasanta Kumar and Guest, David and Knoblich, Andrew J. and Malik, Ashish and Paauwe, Jaap and Papagiannidis, Savvas and Patel, Charmi and Pereira, Vijay and Ren, Shuang and Rogelberg, Steven and Saunders, Mark N. K. and Tung, Rosalie L. and Varma, Arup},
	month = jul,
	year = {2023},
	pages = {606--659},
	file = {Full Text:/Users/daniel/Zotero/storage/8ZUBI86Q/Budhwar et al. - 2023 - Human resource management in the age of generative.pdf:application/pdf},
}

@article{mehrabi_survey_2022,
	title = {A {Survey} on {Bias} and {Fairness} in {Machine} {Learning}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3457607},
	doi = {10.1145/3457607},
	abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
	language = {en},
	number = {6},
	urldate = {2023-11-10},
	journal = {ACM Computing Surveys},
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	month = jul,
	year = {2022},
	pages = {1--35},
	file = {Submitted Version:/Users/daniel/Zotero/storage/7MBWIFE8/Mehrabi et al. - 2022 - A Survey on Bias and Fairness in Machine Learning.pdf:application/pdf},
}

@article{veldanda_are_2023,
	title = {Are {Emily} and {Greg} {Still} {More} {Employable} than {Lakisha} and {Jamal}? {Investigating} {Algorithmic} {Hiring} {Bias} in the {Era} of {ChatGPT}},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	shorttitle = {Are {Emily} and {Greg} {Still} {More} {Employable} than {Lakisha} and {Jamal}?},
	url = {https://arxiv.org/abs/2310.05135},
	doi = {10.48550/ARXIV.2310.05135},
	abstract = {Large Language Models (LLMs) such as GPT-3.5, Bard, and Claude exhibit applicability across numerous tasks. One domain of interest is their use in algorithmic hiring, specifically in matching resumes with job categories. Yet, this introduces issues of bias on protected attributes like gender, race and maternity status. The seminal work of Bertrand \&amp; Mullainathan (2003) set the gold-standard for identifying hiring bias via field experiments where the response rate for identical resumes that differ only in protected attributes, e.g., racially suggestive names such as Emily or Lakisha, is compared. We replicate this experiment on state-of-art LLMs (GPT-3.5, Bard, Claude and Llama) to evaluate bias (or lack thereof) on gender, race, maternity status, pregnancy status, and political affiliation. We evaluate LLMs on two tasks: (1) matching resumes to job categories; and (2) summarizing resumes with employment relevant information. Overall, LLMs are robust across race and gender. They differ in their performance on pregnancy status and political affiliation. We use contrastive input decoding on open-source LLMs to uncover potential sources of bias.},
	urldate = {2023-11-10},
	author = {Veldanda, Akshaj Kumar and Grob, Fabian and Thakur, Shailja and Pearce, Hammond and Tan, Benjamin and Karri, Ramesh and Garg, Siddharth},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
	file = {Veldanda et al. - 2023 - Are Emily and Greg Still More Employable than Laki.pdf:/Users/daniel/Zotero/storage/BZNJNB8A/Veldanda et al. - 2023 - Are Emily and Greg Still More Employable than Laki.pdf:application/pdf},
}

@inproceedings{kotek_gender_2023,
	address = {Delft Netherlands},
	title = {Gender bias and stereotypes in {Large} {Language} {Models}},
	isbn = {9798400701139},
	url = {https://dl.acm.org/doi/10.1145/3582269.3615599},
	doi = {10.1145/3582269.3615599},
	language = {en},
	urldate = {2023-11-23},
	booktitle = {Proceedings of {The} {ACM} {Collective} {Intelligence} {Conference}},
	publisher = {ACM},
	author = {Kotek, Hadas and Dockum, Rikker and Sun, David},
	month = nov,
	year = {2023},
	pages = {12--24},
	file = {Submitted Version:/Users/daniel/Zotero/storage/BXWCMWEE/Kotek et al. - 2023 - Gender bias and stereotypes in Large Language Mode.pdf:application/pdf},
}

@techreport{acerbi_large_2023,
	type = {preprint},
	title = {Large language models show human-like content biases in transmission chain experiments},
	url = {https://osf.io/8zg4d},
	abstract = {As the use of Large Language Models (LLMs) grows, it is important to examine if they exhibit biases in their output. Research in Cultural Evolution, using transmission chain experiments, demonstrates that humans have biases to attend to, remember, and transmit some types of content over others. Here, in five pre-registered experiments with the same methodology, we find that the LLM chatGPT-3 shows biases analogous to humans for content that is gender-stereotype consistent, social, negative, threat-related, and biologically counterintuitive, over other content. The presence of these biases in LLM output suggests that such content is widespread in its training data, and could have consequential downstream effects, by magnifying pre-existing human tendencies for cognitively appealing, and not necessarily informative, or valuable, content.},
	urldate = {2023-11-23},
	institution = {Open Science Framework},
	author = {Acerbi, Alberto and Stubbersfield, Joseph Michael},
	month = jul,
	year = {2023},
	doi = {10.31219/osf.io/8zg4d},
	file = {Submitted Version:/Users/daniel/Zotero/storage/UNYXF7KZ/Acerbi and Stubbersfield - 2023 - Large language models show human-like content bias.pdf:application/pdf},
}

@article{malik_hierarchy_2020,
	title = {A {Hierarchy} of {Limitations} in {Machine} {Learning}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2002.05193},
	doi = {10.48550/ARXIV.2002.05193},
	abstract = {"All models are wrong, but some are useful", wrote George E. P. Box (1979). Machine learning has focused on the usefulness of probability models for prediction in social systems, but is only now coming to grips with the ways in which these models are wrong---and the consequences of those shortcomings. This paper attempts a comprehensive, structured overview of the specific conceptual, procedural, and statistical limitations of models in machine learning when applied to society. Machine learning modelers themselves can use the described hierarchy to identify possible failure points and think through how to address them, and consumers of machine learning models can know what to question when confronted with the decision about if, where, and how to apply machine learning. The limitations go from commitments inherent in quantification itself, through to showing how unmodeled dependencies can lead to cross-validation being overly optimistic as a way of assessing model performance.},
	urldate = {2023-11-28},
	author = {Malik, Momin M.},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Computers and Society (cs.CY), Econometrics (econ.EM), FOS: Economics and business, FOS: Mathematics, G.3; I.6.4; J.4, Machine Learning (stat.ML), Statistics Theory (math.ST)},
	file = {Malik - 2020 - A Hierarchy of Limitations in Machine Learning.pdf:/Users/daniel/Zotero/storage/MC7AFF8T/Malik - 2020 - A Hierarchy of Limitations in Machine Learning.pdf:application/pdf},
}

@misc{tamkin_evaluating_2023,
	title = {Evaluating and {Mitigating} {Discrimination} in {Language} {Model} {Decisions}},
	url = {http://arxiv.org/abs/2312.03689},
	doi = {10.48550/arXiv.2312.03689},
	abstract = {As language models (LMs) advance, interest is growing in applying them to high-stakes societal decisions, such as determining financing or housing eligibility. However, their potential for discrimination in such contexts raises ethical concerns, motivating the need for better methods to evaluate these risks. We present a method for proactively evaluating the potential discriminatory impact of LMs in a wide range of use cases, including hypothetical use cases where they have not yet been deployed. Specifically, we use an LM to generate a wide array of potential prompts that decision-makers may input into an LM, spanning 70 diverse decision scenarios across society, and systematically vary the demographic information in each prompt. Applying this methodology reveals patterns of both positive and negative discrimination in the Claude 2.0 model in select settings when no interventions are applied. While we do not endorse or permit the use of language models to make automated decisions for the high-risk use cases we study, we demonstrate techniques to significantly decrease both positive and negative discrimination through careful prompt engineering, providing pathways toward safer deployment in use cases where they may be appropriate. Our work enables developers and policymakers to anticipate, measure, and address discrimination as language model capabilities and applications continue to expand. We release our dataset and prompts at https://huggingface.co/datasets/Anthropic/discrim-eval},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Tamkin, Alex and Askell, Amanda and Lovitt, Liane and Durmus, Esin and Joseph, Nicholas and Kravec, Shauna and Nguyen, Karina and Kaplan, Jared and Ganguli, Deep},
	month = dec,
	year = {2023},
	note = {arXiv:2312.03689 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/IAU85WPE/Tamkin et al. - 2023 - Evaluating and Mitigating Discrimination in Langua.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/G8EPDXAB/2312.html:text/html},
}

@misc{bandy_problematic_2021,
	title = {Problematic {Machine} {Behavior}: {A} {Systematic} {Literature} {Review} of {Algorithm} {Audits}},
	shorttitle = {Problematic {Machine} {Behavior}},
	url = {http://arxiv.org/abs/2102.04256},
	doi = {10.48550/arXiv.2102.04256},
	abstract = {While algorithm audits are growing rapidly in commonality and public importance, relatively little scholarly work has gone toward synthesizing prior work and strategizing future research in the area. This systematic literature review aims to do just that, following PRISMA guidelines in a review of over 500 English articles that yielded 62 algorithm audit studies. The studies are synthesized and organized primarily by behavior (discrimination, distortion, exploitation, and misjudgement), with codes also provided for domain (e.g. search, vision, advertising, etc.), organization (e.g. Google, Facebook, Amazon, etc.), and audit method (e.g. sock puppet, direct scrape, crowdsourcing, etc.). The review shows how previous audit studies have exposed public-facing algorithms exhibiting problematic behavior, such as search algorithms culpable of distortion and advertising algorithms culpable of discrimination. Based on the studies reviewed, it also suggests some behaviors (e.g. discrimination on the basis of intersectional identities), domains (e.g. advertising algorithms), methods (e.g. code auditing), and organizations (e.g. Twitter, TikTok, LinkedIn) that call for future audit attention. The paper concludes by offering the common ingredients of successful audits, and discussing algorithm auditing in the context of broader research working toward algorithmic justice.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Bandy, Jack},
	month = feb,
	year = {2021},
	note = {arXiv:2102.04256 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/97D6V5VP/Bandy - 2021 - Problematic Machine Behavior A Systematic Literat.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/NXUT3P6B/2102.html:text/html},
}

@misc{creel_algorithmic_2021,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {The {Algorithmic} {Leviathan}: {Arbitrariness}, {Fairness}, and {Opportunity} in {Algorithmic} {Decision} {Making} {Systems}},
	shorttitle = {The {Algorithmic} {Leviathan}},
	url = {https://papers.ssrn.com/abstract=3786377},
	abstract = {This article examines the complaint that arbitrary algorithmic decisions wrong those whom they affect.  It makes three contributions.  First, it provides an analysis of what “arbitrariness” means in this context.  Second, it argues that arbitrariness is not of moral concern except when special circumstances apply. However, when the same algorithm or different algorithms based on the same data, are used in multiple contexts, a person may be arbitrarily excluded from a broad range of opportunities.  The third contribution is to explain why this systemic exclusion is of moral concern and to offer a solution to address it.},
	language = {en},
	urldate = {2023-12-19},
	author = {Creel, Kathleen and Hellman, Deborah},
	month = feb,
	year = {2021},
	keywords = {SSRN, Fairness, and Opportunity in Algorithmic Decision Making Systems, Deborah Hellman, Kathleen Creel, The Algorithmic Leviathan: Arbitrariness},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/E9Z25JYK/Creel and Hellman - 2021 - The Algorithmic Leviathan Arbitrariness, Fairness.pdf:application/pdf},
}

@misc{si_prompting_2023,
	title = {Prompting {GPT}-3 {To} {Be} {Reliable}},
	url = {http://arxiv.org/abs/2210.09150},
	doi = {10.48550/arXiv.2210.09150},
	abstract = {Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Si, Chenglei and Gan, Zhe and Yang, Zhengyuan and Wang, Shuohang and Wang, Jianfeng and Boyd-Graber, Jordan and Wang, Lijuan},
	month = feb,
	year = {2023},
	note = {arXiv:2210.09150 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/RLRVUHWL/Si et al. - 2023 - Prompting GPT-3 To Be Reliable.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/QBE7WFCB/2210.html:text/html},
}

@misc{ganguli_capacity_2023,
	title = {The {Capacity} for {Moral} {Self}-{Correction} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2302.07459},
	doi = {10.48550/arXiv.2302.07459},
	abstract = {We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to "morally self-correct" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Ganguli, Deep and Askell, Amanda and Schiefer, Nicholas and Liao, Thomas I. and Lukošiūtė, Kamilė and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and Olsson, Catherine and Hernandez, Danny and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kernion, Jackson and Kerr, Jamie and Mueller, Jared and Landau, Joshua and Ndousse, Kamal and Nguyen, Karina and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Mercado, Noemi and DasSarma, Nova and Rausch, Oliver and Lasenby, Robert and Larson, Robin and Ringer, Sam and Kundu, Sandipan and Kadavath, Saurav and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and Telleen-Lawton, Timothy and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Olah, Christopher and Clark, Jack and Bowman, Samuel R. and Kaplan, Jared},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07459 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/IR3RQ44P/Ganguli et al. - 2023 - The Capacity for Moral Self-Correction in Large La.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/E2SBMQHG/2302.html:text/html},
}

@misc{solaiman_evaluating_2023,
	title = {Evaluating the {Social} {Impact} of {Generative} {AI} {Systems} in {Systems} and {Society}},
	url = {http://arxiv.org/abs/2306.05949},
	doi = {10.48550/arXiv.2306.05949},
	abstract = {Generative AI systems across modalities, ranging from text, image, audio, and video, have broad social impacts, but there exists no official standard for means of evaluating those impacts and which impacts should be evaluated. We move toward a standard approach in evaluating a generative AI system for any modality, in two overarching categories: what is able to be evaluated in a base system that has no predetermined application and what is able to be evaluated in society. We describe specific social impact categories and how to approach and conduct evaluations in the base technical system, then in people and society. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to all modalities and analyses of the limitations of existing evaluations serve as a starting point for necessary investment in future evaluations. We offer five overarching categories for what is able to be evaluated in society, each with their own subcategories: trustworthiness and autonomy; inequality, marginalization, and violence; concentration of authority; labor and creativity; and ecosystem and environment. Each subcategory includes recommendations for mitigating harm. We are concurrently crafting an evaluation repository for the AI research community to contribute existing evaluations along the given categories. This version will be updated following a CRAFT session at ACM FAccT 2023.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Solaiman, Irene and Talat, Zeerak and Agnew, William and Ahmad, Lama and Baker, Dylan and Blodgett, Su Lin and Daumé III, Hal and Dodge, Jesse and Evans, Ellie and Hooker, Sara and Jernite, Yacine and Luccioni, Alexandra Sasha and Lusoli, Alberto and Mitchell, Margaret and Newman, Jessica and Png, Marie-Therese and Strait, Andrew and Vassilev, Apostol},
	month = jun,
	year = {2023},
	note = {arXiv:2306.05949 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/C7JJ28A8/Solaiman et al. - 2023 - Evaluating the Social Impact of Generative AI Syst.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/6JP3RANW/2306.html:text/html},
}

@misc{zhao_explainability_2023,
	title = {Explainability for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Explainability for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.01029},
	doi = {10.48550/arXiv.2309.01029},
	abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional machine learning models.},
	urldate = {2023-12-22},
	publisher = {arXiv},
	author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
	month = nov,
	year = {2023},
	note = {arXiv:2309.01029 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/FAA63ICS/Zhao et al. - 2023 - Explainability for Large Language Models A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/IRSUTCBQ/2309.html:text/html},
}

@incollection{gebru_race_2020,
	title = {Race and {Gender}},
	isbn = {978-0-19-006739-7},
	url = {https://doi.org/10.1093/oxfordhb/9780190067397.013.16},
	abstract = {This chapter discusses the role of race and gender in artificial intelligence (AI). The rapid permeation of AI into society has not been accompanied by a thorough investigation of the sociopolitical issues that cause certain groups of people to be harmed rather than advantaged by it. For instance, recent studies have shown that commercial automated facial analysis systems have much higher error rates for dark-skinned women, while having minimal errors on light-skinned men. Moreover, a 2016 ProPublica investigation uncovered that machine learning–based tools that assess crime recidivism rates in the United States are biased against African Americans. Other studies show that natural language–processing tools trained on news articles exhibit societal biases. While many technical solutions have been proposed to alleviate bias in machine learning systems, a holistic and multifaceted approach must be taken. This includes standardization bodies determining what types of systems can be used in which scenarios, making sure that automated decision tools are created by people from diverse backgrounds, and understanding the historical and political factors that disadvantage certain groups who are subjected to these tools.},
	urldate = {2023-12-26},
	booktitle = {The {Oxford} {Handbook} of {Ethics} of {AI}},
	publisher = {Oxford University Press},
	author = {Gebru, Timnit},
	editor = {Dubber, Markus D. and Pasquale, Frank and Das, Sunit},
	month = jul,
	year = {2020},
	doi = {10.1093/oxfordhb/9780190067397.013.16},
	pages = {0},
	file = {Gebru - 2020 - Race and Gender.pdf:/Users/daniel/Zotero/storage/KQLQMJS4/Gebru - 2020 - Race and Gender.pdf:application/pdf;Snapshot:/Users/daniel/Zotero/storage/2W5R84ZD/290662826.html:text/html},
}

@article{power_performing_2022,
	title = {Performing the ‘good tenant’},
	volume = {37},
	issn = {0267-3037},
	url = {https://doi.org/10.1080/02673037.2020.1813260},
	doi = {10.1080/02673037.2020.1813260},
	abstract = {Renters in homeowner societies like Australia, the United States and United Kingdom occupy a complex moral landscape, maligned for failure to achieve homeownership but pivotal to the value of investment properties. Identification of ‘good’ and ‘risky’ tenants is an important landlord practice. We investigate how tenants conceptualise and perform the ‘good tenant’ through research with 36 single older women renting in greater Sydney, Australia: a cohort on the margins of secure housing. The good tenant demonstrates responsibility through paying rent on time and property stewardship (reporting repairs, making home). However, these practices are made necessary and risky through limited tenure security. The emotional and financial risks attending performances of the good tenant drive paradoxical relations; a good tenant is also acquiescent and silent, not reporting property repairs or lapsed leases to avoid rent increases and/or evictions. Variegated performances of the ‘good tenant’ reflect cultural property norms and valorize the investment function of housing yet could also productively unsettle tenant-landlord relations.},
	number = {3},
	urldate = {2023-12-28},
	journal = {Housing Studies},
	author = {Power, Emma R. and Gillon, Charles},
	month = mar,
	year = {2022},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/02673037.2020.1813260},
	keywords = {Australia, home, housing, Renting, responsibility, risk},
	pages = {459--482},
	file = {Power and Gillon - 2022 - Performing the ‘good tenant’.pdf:/Users/daniel/Zotero/storage/XAXMZGPK/Power and Gillon - 2022 - Performing the ‘good tenant’.pdf:application/pdf},
}

@article{awad_artificial_2023,
	title = {Artificial {Intelligence} and {Debiasing} in {Hiring}: {Impact} on {Applicant} {Quality} and {Gender} {Diversity}},
	shorttitle = {Artificial {Intelligence} and {Debiasing} in {Hiring}},
	url = {https://papers.ssrn.com/abstract=4626059},
	doi = {10.2139/ssrn.4626059},
	abstract = {The use of Artificial Intelligence (AI) in hiring is becoming increasingly popular, but little is known about the direct impact of its use on applicant decision},
	language = {en},
	urldate = {2023-12-29},
	author = {Awad, Edmond and Balafoutas, Loukas and Chen, Li and Ip, Edwin and Vecci, Joe},
	month = nov,
	year = {2023},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/4Q3Y3TQG/Awad et al. - 2023 - Artificial Intelligence and Debiasing in Hiring I.pdf:application/pdf},
}

@misc{koh_bad_2023,
	title = {{BAD}: {BiAs} {Detection} for {Large} {Language} {Models} in the context of candidate screening},
	shorttitle = {{BAD}},
	url = {http://arxiv.org/abs/2305.10407},
	doi = {10.48550/arXiv.2305.10407},
	abstract = {Application Tracking Systems (ATS) have allowed talent managers, recruiters, and college admissions committees to process large volumes of potential candidate applications efficiently. Traditionally, this screening process was conducted manually, creating major bottlenecks due to the quantity of applications and introducing many instances of human bias. The advent of large language models (LLMs) such as ChatGPT and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed. In this project, we wish to identify and quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the context of candidate screening in order to demonstrate how the use of these models could perpetuate existing biases and inequalities in the hiring process.},
	urldate = {2023-12-29},
	publisher = {arXiv},
	author = {Koh, Nam Ho and Plata, Joseph and Chai, Joyce},
	month = may,
	year = {2023},
	note = {arXiv:2305.10407 [cs]},
	keywords = {Computer Science - Computation and Language, F.2.2, I.2.7, I.2, I.2.7},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/MMGIQHP6/Koh et al. - 2023 - BAD BiAs Detection for Large Language Models in t.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/U3YLQBHV/2305.html:text/html},
}

@article{pisanelli_your_2022-1,
	title = {Your resume is your gatekeeper: {Automated} resume screening as a strategy to reduce gender gaps in hiring},
	volume = {221},
	issn = {0165-1765},
	shorttitle = {Your resume is your gatekeeper},
	url = {https://www.sciencedirect.com/science/article/pii/S0165176522003664},
	doi = {10.1016/j.econlet.2022.110892},
	abstract = {Firms increasingly rely on artificial intelligence (AI) algorithms for hiring. The literature prompts concerns about such AI algorithms hindering gender equality in employment outcomes. Using a unique field study, I find human recruiters’ gender stereotypes lead to women having 69\% lower chances of being interviewed for a gender-neutral job, compared to equally qualified men. Introducing automated resume screening shrinks such a gender gap by 43 percentage points.},
	urldate = {2023-12-29},
	journal = {Economics Letters},
	author = {Pisanelli, Elena},
	month = dec,
	year = {2022},
	keywords = {Gender, Inequality, Artificial intelligence, Hiring},
	pages = {110892},
	file = {Pisanelli - 2022 - Your resume is your gatekeeper Automated resume s.pdf:/Users/daniel/Zotero/storage/UWJ6T6K2/Pisanelli - 2022 - Your resume is your gatekeeper Automated resume s.pdf:application/pdf;ScienceDirect Snapshot:/Users/daniel/Zotero/storage/F4UFIAVN/S0165176522003664.html:text/html},
}

@article{quillian_comparative_2021,
	title = {Comparative {Perspectives} on {Racial} {Discrimination} in {Hiring}: {The} {Rise} of {Field} {Experiments}},
	volume = {47},
	shorttitle = {Comparative {Perspectives} on {Racial} {Discrimination} in {Hiring}},
	url = {https://doi.org/10.1146/annurev-soc-090420-035144},
	doi = {10.1146/annurev-soc-090420-035144},
	abstract = {This article reviews studies of hiring discrimination against racial and ethnic minority groups in cross-national perspective. We focus on field experimental studies of hiring discrimination: studies that use fictitious applications from members of different racial and ethnic groups to apply for actual jobs. There are more than 140 field experimental studies of hiring discrimination against ethno-racial minority groups in 30 countries. We outline seventeen empirical findings from this body of studies. We also discuss individual and contextual theories of hiring discrimination, the relative strengths and weaknesses of field experiments to assess discrimination, and the history of such field experiments. The comparative scope of this body of research helps to move beyond micromodels of employer decision-making to better understand the roles of history, social context, institutional rules, and racist ideologies in producing discrimination. These studies show that racial and ethnic discrimination is a pervasive international phenomenon that has hardly declined over time, although levels vary significantly over countries. Evidence indicates that institutional rules regarding race and ethnicity in hiring can have an important influence on levels of discrimination. Suggestions for future research on discrimination are discussed.},
	number = {1},
	urldate = {2023-12-29},
	journal = {Annual Review of Sociology},
	author = {Quillian, Lincoln and Midtbøen, Arnfinn H.},
	year = {2021},
	note = {\_eprint: https://doi.org/10.1146/annurev-soc-090420-035144},
	keywords = {discrimination, ethnicity, field experiments, labor markets, race, racism},
	pages = {391--415},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/SJM3CFN8/Quillian and Midtbøen - 2021 - Comparative Perspectives on Racial Discrimination .pdf:application/pdf},
}

@article{quillian_trends_2023,
	title = {Trends in racial and ethnic discrimination in hiring in six {Western} countries},
	volume = {120},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.2212875120},
	doi = {10.1073/pnas.2212875120},
	abstract = {We examine trends in racial and ethnic discrimination in hiring in six European and North American countries: Canada, France, Germany, Great Britain, the Netherlands, and the United States. Our sample includes all available discrimination estimates from 90 field experimental studies of hiring discrimination, encompassing more than 170,000 applications for jobs. The years covered vary by country, ranging from 1969 to 2017 for Great Britain to 1994 to 2017 for Germany. We examine trends in discrimination against four racial-ethnic origin groups: African/Black, Asian, Latin American/Hispanic, and Middle Eastern or North African. The results indicate that levels of discrimination in callbacks have remained either unchanged or slightly increased overall for most countries and origin categories. There are three notable exceptions. First, hiring discrimination against ethnic groups with origins in the Middle East and North Africa increased during the 2000s relative to the 1990s. Second, we find that discrimination in France declined, although from very high to “merely” high levels. Third, we find evidence that discrimination in the Netherlands has increased over time. Controls for study characteristics do not change these trends. Contrary to the idea that discrimination will tend to decline in Western countries, we find that discrimination has not fallen over the last few decades in five of the six Western countries we examine.},
	number = {6},
	urldate = {2023-12-30},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Quillian, Lincoln and Lee, John J.},
	month = feb,
	year = {2023},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2212875120},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/RS3HI79S/Quillian and Lee - 2023 - Trends in racial and ethnic discrimination in hiri.pdf:application/pdf},
}

@article{karlson_how_2023,
	title = {How ({Not}) to {Use} {Risk} {Ratios} in {Sociological} {Research}},
	volume = {9},
	issn = {2378-0231},
	url = {https://doi.org/10.1177/23780231231192394},
	doi = {10.1177/23780231231192394},
	abstract = {A small but growing literature uses the risk ratio as an association or effect measure. Unlike odds ratios, risk ratios are unaffected by rescaling or noncollapsibility bias and are straightforward to interpret. However, the risk ratio has one unattractive property that researchers need to be aware of: it is not symmetric with respect to the outcome definition. The ratio between two groups’ probability of success does not equal the inverse of the ratio between the two groups’ probability of failure. Choosing the category of a binary outcome to use as the “success” category can significantly affect substantive conclusions, particularly in research comparing risk ratios with highly different base rates of the “success” outcome. The authors give examples from discrimination and social mobility studies that illustrate this point and present rules of thumb for the use of the risk ratio depending on the base rate of the outcome.},
	language = {en},
	urldate = {2024-01-05},
	journal = {Socius},
	author = {Karlson, Kristian Bernt and Quillian, Lincoln},
	month = jan,
	year = {2023},
	note = {Publisher: SAGE Publications},
	pages = {23780231231192394},
	file = {SAGE PDF Full Text:/Users/daniel/Zotero/storage/R4ETY8PK/Karlson and Quillian - 2023 - How (Not) to Use Risk Ratios in Sociological Resea.pdf:application/pdf},
}

@article{ghekiere_perception_2023,
	title = {The perception of names in experimental studies on ethnic origin: a cross-national validation in {Europe}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {The perception of names in experimental studies on ethnic origin},
	url = {https://osf.io/hswg6/},
	doi = {10.17605/OSF.IO/HSWG6},
	abstract = {In researching ethnic discrimination in crucial life domains, one needs a signal for ethnic origin: names and pictures. The ethnic origin of candidates is often signalled through name- and picture-based treatments in correspondence studies on discrimination, conjoint experiments or implicit association tests (Gaddis 2018; Verhaeghe 2022). Consequently, the level of discrimination or prejudice measured in these studies heavily depends on the names and pictures selected. The use of name treatments relies on the assumption that people can correctly identify the ethnicity or national origin of different names. However, it turns out that this ‘correct’ perception is often assumed (Martiniello \&amp; Verhaeghe 2022) and appears to depend on the specific national or regional context (Tuppat \&amp; Gerhards 2021; Martiniello \&amp; Verhaeghe 2022; Fernández-Reino \&amp; Creighton 2023). Moreover, names not only signal ethnic origin, but also other characteristics - such as social class, religiosity and gender- which might affect the behaviour of realtors, employers in correspondence audits (e.g., Butler \&amp; Homola 2017; Gaddis 2017; Martiniello \&amp; Verhaeghe 2022; Crabtree et al 2022; Fernández-Reino \&amp; Creighton 2023; Elder \&amp; Hayes 2023). This pre-registration plan presents a survey instrument to measure public perceptions of names signalling different national and ethnic origins. The aim of this name perceptions survey is to create a database of names that can be validly used in experimental studies, such as correspondence audits, to signal ethnicity and national origin in Europe. This survey is part of the EqualStrength project, that will rely on this database to signal ethnicity and national origin in correspondence audits and survey experiments (https://equalstrength.eu/).},
	urldate = {2024-01-06},
	author = {Ghekiere, Abel and Bori Simonovits and Zey, Elli and Hildebrandt, Johanna and Steinmetz, Stephanie and Lázaro, Isabel and Kuhnle, Jeremy and Van Oosten, Sanne and Zschirnt, Eva and Lancee, Bram and Lachkovska, Vasilena and Vass-Vigh, Veronika and Atay, Pelin and Di Stasio, Valentina and Scharle, Ágota and Ketevani Kapanadze and Fernández-Reino, Mariña and Montag, Josef and Sprong, Stefanie and Capistrano, Daniel and Veit, Susanne and Ely Strömberg and Boado, Héctor Cebolla and Martiniello, Billie and Fossati, Flavia and Suarez, Alvaro and Verhaeghe, Pieter-Paul and Creighton, Mathew},
	collaborator = {{Center For Open Science}},
	year = {2023},
	note = {Publisher: OSF Registries},
	keywords = {Discrimination, Sociology, Audit study, FOS: Sociology, Inequality and Stratification, Name selection, Social and Behavioral Sciences},
}

@misc{lancee_gemm_2019,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {The {GEMM} {Study}: {A} {Cross}-{National} {Harmonized} {Field} {Experiment} on {Labour} {Market} {Discrimination}: {Technical} {Report}},
	shorttitle = {The {GEMM} {Study}},
	url = {https://papers.ssrn.com/abstract=3398191},
	doi = {10.2139/ssrn.3398191},
	abstract = {This paper describes and documents the data collection of the GEMM project: A cross-national harmonized correspondence study on ethnic discrimination in hiring. Data is collected in Germany, Norway, The Netherlands, Britain, and Spain. The paper describes the research design, the experimental manipulations, the procedure of data collection and the ethical considerations. Also discussed are the occupations included, and the search procedure for vacancies. In sum, the technical report gives a detailed account of the collection of the GEMM data.},
	language = {en},
	urldate = {2024-01-06},
	author = {Lancee, Bram and Birkelund, Gunn and Coenders, Marcel and Di Stasio, Valentina and Fernandez Reino, Marina and Heath, Anthony and Koopmans, Ruud and Larsen, Edvard and Polavieja, Javier G. and Ramos, Maria and Thijssen, Lex and Veit, Susanne and Yemane, Ruta and Zwier, Dieuwke},
	month = jun,
	year = {2019},
	keywords = {ethnic discrimination, employers, field experiments, cross-national comparison, labour market, technical report},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/X4W537L2/Lancee et al. - 2019 - The GEMM Study A Cross-National Harmonized Field .pdf:application/pdf},
}

@techreport{lane_impact_2023,
	address = {Paris},
	title = {The impact of {AI} on the workplace: {Main} findings from the {OECD} {AI} surveys of employers and workers},
	shorttitle = {The impact of {AI} on the workplace},
	url = {https://www.oecd-ilibrary.org/social-issues-migration-health/the-impact-of-ai-on-the-workplace-main-findings-from-the-oecd-ai-surveys-of-employers-and-workers_ea0a0fe1-en},
	abstract = {New OECD surveys of employers and workers in the manufacturing and finance sectors of seven countries shed new light on the impact that Artificial Intelligence has on the workplace —an under-researched area to date due to lack of data. The findings suggest that both workers and their employers are generally very positive about the impact of AI on performance and working conditions. However, there are also concerns, including about job loss—an issue that should be closely monitored. The surveys also indicate that, while many workers trust their employers when it comes to the implementation of AI in the workplace, more can be done to improve trust. In particular, the surveys show that both training and worker consultation are associated with better outcomes for workers.},
	language = {en},
	urldate = {2024-01-20},
	institution = {OECD},
	author = {Lane, Marguerita and Williams, Morgan and Broecke, Stijn},
	month = mar,
	year = {2023},
	doi = {10.1787/ea0a0fe1-en},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/V9MINTHP/Lane et al. - 2023 - The impact of AI on the workplace Main findings f.pdf:application/pdf},
}

@book{barocas-hardt-narayanan,
	title = {Fairness and machine learning: {Limitations} and opportunities},
	publisher = {MIT Press},
	author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
	year = {2023},
	file = {Barocas et al. - Fairness and Machine Learning.pdf:/Users/daniel/Zotero/storage/PAJVZC8U/Barocas et al. - Fairness and Machine Learning.pdf:application/pdf},
}

@article{mellon_ais_2024,
	title = {Do {AIs} know what the most important issue is? {Using} language models to code open-text social survey responses at scale},
	volume = {11},
	issn = {2053-1680},
	shorttitle = {Do {AIs} know what the most important issue is?},
	url = {https://doi.org/10.1177/20531680241231468},
	doi = {10.1177/20531680241231468},
	abstract = {Can artificial intelligence accurately label open-text survey responses? We compare the accuracy of six large language models (LLMs) using a few-shot approach, three supervised learning algorithms (SVM, DistilRoBERTa, and a neural network trained on BERT embeddings), and a second human coder on the task of categorizing “most important issue” responses from the British Election Study Internet Panel into 50 categories. For the scenario where a researcher lacks existing training data, the accuracy of the highest-performing LLM (Claude-1.3: 93.9\%) neared human performance (94.7\%) and exceeded the highest-performing supervised approach trained on 1000 randomly sampled cases (neural network: 93.5\%). In a scenario where previous data has been labeled but a researcher wants to label novel text, the best LLM’s (Claude-1.3: 80.9\%) few-shot performance is only slightly behind the human (88.6\%) and exceeds the best supervised model trained on 576,000 cases (DistilRoBERTa: 77.8\%). PaLM-2, Llama-2, and the SVM all performed substantially worse than the best LLMs and supervised models across all metrics and scenarios. Our results suggest that LLMs may allow for greater use of open-ended survey questions in the future.},
	language = {en},
	number = {1},
	urldate = {2024-02-13},
	journal = {Research \& Politics},
	author = {Mellon, Jonathan and Bailey, Jack and Scott, Ralph and Breckwoldt, James and Miori, Marta and Schmedeman, Phillip},
	month = jan,
	year = {2024},
	note = {Publisher: SAGE Publications Ltd},
	pages = {20531680241231468},
	file = {SAGE PDF Full Text:/Users/daniel/Zotero/storage/GP32MSEH/Mellon et al. - 2024 - Do AIs know what the most important issue is Usin.pdf:application/pdf},
}

@article{kordzadeh_algorithmic_2022,
	title = {Algorithmic bias: review, synthesis, and future research directions},
	volume = {31},
	issn = {0960-085X},
	shorttitle = {Algorithmic bias},
	url = {https://doi.org/10.1080/0960085X.2021.1927212},
	doi = {10.1080/0960085X.2021.1927212},
	abstract = {As firms are moving towards data-driven decision making, they are facing an emerging problem, namely, algorithmic bias. Accordingly, algorithmic systems can yield socially-biased outcomes, thereby compounding inequalities in the workplace and in society. This paper reviews, summarises, and synthesises the current literature related to algorithmic bias and makes recommendations for future information systems research. Our literature analysis shows that most studies have conceptually discussed the ethical, legal, and design implications of algorithmic bias, whereas only a limited number have empirically examined them. Moreover, the mechanisms through which technology-driven biases translate into decisions and behaviours have been largely overlooked. Based on the reviewed papers and drawing on theories such as the stimulus-organism-response theory and organisational justice theory, we identify and explicate eight important theoretical concepts and develop a research model depicting the relations between those concepts. The model proposes that algorithmic bias can affect fairness perceptions and technology-related behaviours such as machine-generated recommendation acceptance, algorithm appreciation, and system adoption. The model also proposes that contextual dimensions (i.e., individual, task, technology, organisational, and environmental) can influence the perceptual and behavioural manifestations of algorithmic bias. These propositions highlight the significant gap in the literature and provide a roadmap for future studies.},
	number = {3},
	urldate = {2024-02-22},
	journal = {European Journal of Information Systems},
	author = {Kordzadeh, Nima and Ghasemaghaei, Maryam},
	month = may,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0960085X.2021.1927212},
	keywords = {ai ethics, algorithmic accountability, Algorithmic bias, algorithmic fairness, data analytics, Patrick Mikalef, Aleš Popovic, Jenny Eriksson Lundström and Kieran Conboy, responsible ai},
	pages = {388--409},
	file = {Kordzadeh and Ghasemaghaei - 2022 - Algorithmic bias review, synthesis, and future re.pdf:/Users/daniel/Zotero/storage/95IZU6QB/Kordzadeh and Ghasemaghaei - 2022 - Algorithmic bias review, synthesis, and future re.pdf:application/pdf},
}

@article{hall_systematic_2023,
	title = {A systematic review of socio-technical gender bias in {AI} algorithms},
	volume = {47},
	issn = {1468-4527},
	url = {https://doi.org/10.1108/OIR-08-2021-0452},
	doi = {10.1108/OIR-08-2021-0452},
	abstract = {Purpose Gender bias in artificial intelligence (AI) should be solved as a priority before AI algorithms become ubiquitous, perpetuating and accentuating the bias. While the problem has been identified as an established research and policy agenda, a cohesive review of existing research specifically addressing gender bias from a socio-technical viewpoint is lacking. Thus, the purpose of this study is to determine the social causes and consequences of, and proposed solutions to, gender bias in AI algorithms. Design/methodology/approach A comprehensive systematic review followed established protocols to ensure accurate and verifiable identification of suitable articles. The process revealed 177 articles in the socio-technical framework, with 64 articles selected for in-depth analysis. Findings Most previous research has focused on technical rather than social causes, consequences and solutions to AI bias. From a social perspective, gender bias in AI algorithms can be attributed equally to algorithmic design and training datasets. Social consequences are wide-ranging, with amplification of existing bias the most common at 28\%. Social solutions were concentrated on algorithmic design, specifically improving diversity in AI development teams (30\%), increasing awareness (23\%), human-in-the-loop (23\%) and integrating ethics into the design process (21\%). Originality/value This systematic review is the first of its kind to focus on gender bias in AI algorithms from a social perspective within a socio-technical framework. Identification of key causes and consequences of bias and the breakdown of potential solutions provides direction for future research and policy within the growing field of AI ethics. Peer review The peer review history for this article is available at https://publons.com/publon/10.1108/OIR-08-2021-0452},
	number = {7},
	urldate = {2024-02-22},
	journal = {Online Information Review},
	author = {Hall, Paula and Ellis, Debbie},
	month = jan,
	year = {2023},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {Algorithmic bias, AI ethics, AI gender Bias, Gender bias, Socio-technical framework},
	pages = {1264--1279},
	file = {Hall and Ellis - 2023 - A systematic review of socio-technical gender bias.pdf:/Users/daniel/Zotero/storage/Y84DWX27/Hall and Ellis - 2023 - A systematic review of socio-technical gender bias.pdf:application/pdf},
}

@article{landers_auditing_2023,
	title = {Auditing the {AI} auditors: {A} framework for evaluating fairness and bias in high stakes {AI} predictive models.},
	volume = {78},
	issn = {1935-990X, 0003-066X},
	shorttitle = {Auditing the {AI} auditors},
	url = {https://doi.apa.org/doi/10.1037/amp0000972},
	doi = {10.1037/amp0000972},
	language = {en},
	number = {1},
	urldate = {2024-02-23},
	journal = {American Psychologist},
	author = {Landers, Richard N. and Behrend, Tara S.},
	month = jan,
	year = {2023},
	pages = {36--49},
	file = {Landers and Behrend - 2023 - Auditing the AI auditors A framework for evaluati.pdf:/Users/daniel/Zotero/storage/GUKC9T8R/Landers and Behrend - 2023 - Auditing the AI auditors A framework for evaluati.pdf:application/pdf},
}

@article{ferrara_should_2023,
	title = {Should {ChatGPT} be {Biased}? {Challenges} and {Risks} of {Bias} in {Large} {Language} {Models}},
	issn = {1396-0466},
	shorttitle = {Should {ChatGPT} be {Biased}?},
	url = {http://arxiv.org/abs/2304.03738},
	doi = {10.5210/fm.v28i11.13346},
	abstract = {As the capabilities of generative language models continue to advance, the implications of biases ingrained within these models have garnered increasing attention from researchers, practitioners, and the broader public. This article investigates the challenges and risks associated with biases in large-scale language models like ChatGPT. We discuss the origins of biases, stemming from, among others, the nature of training data, model specifications, algorithmic constraints, product design, and policy decisions. We explore the ethical concerns arising from the unintended consequences of biased model outputs. We further analyze the potential opportunities to mitigate biases, the inevitability of some biases, and the implications of deploying these models in various applications, such as virtual assistants, content generation, and chatbots. Finally, we review the current approaches to identify, quantify, and mitigate biases in language models, emphasizing the need for a multi-disciplinary, collaborative effort to develop more equitable, transparent, and responsible AI systems. This article aims to stimulate a thoughtful dialogue within the artificial intelligence community, encouraging researchers and developers to reflect on the role of biases in generative language models and the ongoing pursuit of ethical AI.},
	urldate = {2024-02-24},
	journal = {First Monday},
	author = {Ferrara, Emilio},
	month = nov,
	year = {2023},
	note = {arXiv:2304.03738 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/VKASC5WD/Ferrara - 2023 - Should ChatGPT be Biased Challenges and Risks of .pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/87V4QQSM/2304.html:text/html},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/QHMCHBCS/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/MUBT4WRB/2005.html:text/html},
}

@article{grabb_emerging_2023,
	title = {Emerging {Forensic} {Implications} of the {Artificial} {Intelligence} {Revolution}},
	volume = {51},
	copyright = {© 2023 American Academy of Psychiatry and the Law},
	issn = {1093-6793},
	url = {https://jaapl.org/content/51/4/475},
	doi = {10.29158/JAAPL.230080-23},
	abstract = {Artificial intelligence (AI) is changing everything as we know it, and forensic psychiatry is not immune to the change. Academic discussion of the role of AI in the forensic realm has largely focused on suicide risk stratification in the form of machine learning, allowing the technology to sift},
	language = {en},
	number = {4},
	urldate = {2024-02-26},
	journal = {Journal of the American Academy of Psychiatry and the Law Online},
	author = {Grabb, Declan J. and Angelotta, Cara},
	month = dec,
	year = {2023},
	pmid = {38065619},
	note = {Publisher: Journal of the American Academy of Psychiatry and the Law Online
Section: EDITORIAL},
	keywords = {AI, machine learning, risk, forensic},
	pages = {475--479},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/SFJRFFRQ/Grabb and Angelotta - 2023 - Emerging Forensic Implications of the Artificial I.pdf:application/pdf},
}

@article{kasneci_chatgpt_2023,
	title = {{ChatGPT} for good? {On} opportunities and challenges of large language models for education},
	volume = {103},
	issn = {10416080},
	shorttitle = {{ChatGPT} for good?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1041608023000195},
	doi = {10.1016/j.lindif.2023.102274},
	language = {en},
	urldate = {2024-02-27},
	journal = {Learning and Individual Differences},
	author = {Kasneci, Enkelejda and Sessler, Kathrin and Küchemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and Günnemann, Stephan and Hüllermeier, Eyke and Krusche, Stephan and Kutyniok, Gitta and Michaeli, Tilman and Nerdel, Claudia and Pfeffer, Jürgen and Poquet, Oleksandra and Sailer, Michael and Schmidt, Albrecht and Seidel, Tina and Stadler, Matthias and Weller, Jochen and Kuhn, Jochen and Kasneci, Gjergji},
	month = apr,
	year = {2023},
	pages = {102274},
	file = {Submitted Version:/Users/daniel/Zotero/storage/2F4D7ULZ/Kasneci et al. - 2023 - ChatGPT for good On opportunities and challenges .pdf:application/pdf},
}

@incollection{zimmermann_correspondence_2022,
	address = {Cham},
	title = {Correspondence {Studies}},
	isbn = {978-3-319-57365-6},
	url = {https://link.springer.com/10.1007/978-3-319-57365-6_306-1},
	language = {en},
	urldate = {2024-02-27},
	booktitle = {Handbook of {Labor}, {Human} {Resources} and {Population} {Economics}},
	publisher = {Springer International Publishing},
	author = {Verhaeghe, Pieter-Paul},
	editor = {Zimmermann, Klaus F.},
	year = {2022},
	doi = {10.1007/978-3-319-57365-6_306-1},
	pages = {1--19},
}

@misc{bommasani_opportunities_2022,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2108.07258},
	doi = {10.48550/arXiv.2108.07258},
	abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	month = jul,
	year = {2022},
	note = {arXiv:2108.07258 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/K66T7GJI/Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Model.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/6FFKL728/2108.html:text/html},
}

@misc{humlum_adoption_2024,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {The {Adoption} of {ChatGPT}},
	url = {https://papers.ssrn.com/abstract=4807516},
	doi = {10.2139/ssrn.4807516},
	abstract = {We study the adoption of ChatGPT, the icon of Generative AI, using a large-scale survey experiment linked to comprehensive register data in Denmark. Surveying 100,000 workers from 11 exposed occupations, we document ChatGPT is pervasive: half of workers have used it, with younger, less experienced, higher-achieving, and especially male workers leading the curve. Why have some workers adopted ChatGPT, and others not? Workers see a substantial productivity potential in ChatGPT, understand it substitutes for human expertise, and expect little cross-task substitution. Adoption is hindered by practical hurdles, including employer restrictions and required training, rather than existential fears of job redundancy or technology dependency. Informing workers about expert assessments of ChatGPT shifts workers’ beliefs and intentions but has limited impacts on their adoption of the technology.},
	language = {en},
	urldate = {2024-05-13},
	author = {Humlum, Anders and Vestergaard, Emilie},
	month = apr,
	year = {2024},
	keywords = {SSRN, Anders Humlum, Emilie Vestergaard, The Adoption of ChatGPT},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/T5UIKDKE/Humlum and Vestergaard - 2024 - The Adoption of ChatGPT.pdf:application/pdf},
}

@article{kalev_best_2006,
	title = {Best {Practices} or {Best} {Guesses}? {Assessing} the {Efficacy} of {Corporate} {Affirmative} {Action} and {Diversity} {Policies}},
	volume = {71},
	copyright = {http://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {0003-1224, 1939-8271},
	shorttitle = {Best {Practices} or {Best} {Guesses}?},
	url = {http://journals.sagepub.com/doi/10.1177/000312240607100404},
	doi = {10.1177/000312240607100404},
	abstract = {Employers have experimented with three broad approaches to promoting diversity. Some programs are designed to establish organizational responsibility for diversity, others to moderate managerial bias through training and feedback, and still others to reduce the social isolation of women and minority workers. These approaches find support in academic theories of how organizations achieve goals, how stereotyping shapes hiring and promotion, and how networks influence careers. This is the first systematic analysis of their efficacy. The analyses rely on federal data describing the workforces of 708 private sector establishments from 1971 to 2002, coupled with survey data on their employment practices. Efforts to moderate managerial bias through diversity training and diversity evaluations are least effective at increasing the share of white women, black women, and black men in management. Efforts to attack social isolation through mentoring and networking show modest effects. Efforts to establish responsibility for diversity lead to the broadest increases in managerial diversity. Moreover, organizations that establish responsibility see better effects from diversity training and evaluations, networking, and mentoring. Employers subject to federal affirmative action edicts, who typically assign responsibility for compliance to a manager, also see stronger effects from some programs. This work lays the foundation for an institutional theory of the remediation of workplace inequality.},
	language = {en},
	number = {4},
	urldate = {2024-05-13},
	journal = {American Sociological Review},
	author = {Kalev, Alexandra and Dobbin, Frank and Kelly, Erin},
	month = aug,
	year = {2006},
	pages = {589--617},
	file = {Kalev et al. - 2006 - Best Practices or Best Guesses Assessing the Effi.pdf:/Users/daniel/Zotero/storage/ILT9N3YE/Kalev et al. - 2006 - Best Practices or Best Guesses Assessing the Effi.pdf:application/pdf},
}

@article{noon_pointless_2018,
	title = {Pointless {Diversity} {Training}: {Unconscious} {Bias}, {New} {Racism} and {Agency}},
	volume = {32},
	issn = {0950-0170},
	shorttitle = {Pointless {Diversity} {Training}},
	url = {https://doi.org/10.1177/0950017017719841},
	doi = {10.1177/0950017017719841},
	abstract = {The latest fashion of ‘unconscious bias training’ is a diversity intervention based on unproven suppositions and is unlikely to help eliminate racism in the workplace. Knowing about bias does not automatically result in changes in behaviour by managers and employees. Even if ‘unconscious bias training’ has the theoretical potential to change behaviour, it will depend on the type of racism: symbolic/modern/colour-blind, aversive or blatant. In addition, even if those deemed racist are motivated to change behaviour, structural constraints can militate against pro-diversity actions. Agency is overstated by psychology-inspired ‘unconscious bias training’ proponents, leading them to assume the desirability and effectiveness of this type of diversity training intervention, but from a critical diversity perspective (sociologically influenced) the training looks pointless.},
	language = {en},
	number = {1},
	urldate = {2024-05-13},
	journal = {Work, Employment and Society},
	author = {Noon, Mike},
	month = feb,
	year = {2018},
	note = {Publisher: SAGE Publications Ltd},
	pages = {198--209},
	file = {Submitted Version:/Users/daniel/Zotero/storage/4AMP9MMF/Noon - 2018 - Pointless Diversity Training Unconscious Bias, Ne.pdf:application/pdf},
}

@misc{chen_how_2023,
	title = {How is {ChatGPT}'s behavior changing over time?},
	url = {http://arxiv.org/abs/2307.09009},
	doi = {10.48550/arXiv.2307.09009},
	abstract = {GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on several diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3) opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating code, 6) US Medical License tests, and 7) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was reasonable at identifying prime vs. composite numbers (84\% accuracy) but GPT-4 (June 2023) was poor on these same questions (51\% accuracy). This is partly explained by a drop in GPT-4's amenity to follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in June than in March in this task. GPT-4 became less willing to answer sensitive questions and opinion survey questions in June than in March. GPT-4 performed better at multi-hop questions in June than in March, while GPT-3.5's performance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. We provide evidence that GPT-4's ability to follow user instructions has decreased over time, which is one common factor behind the many behavior drifts. Overall, our findings show that the behavior of the "same" LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLMs.},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Chen, Lingjiao and Zaharia, Matei and Zou, James},
	month = oct,
	year = {2023},
	note = {arXiv:2307.09009 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/AY9957I3/Chen et al. - 2023 - How is ChatGPT's behavior changing over time.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/HQG2YHYW/2307.html:text/html},
}

@misc{armstrong_silicon_2024,
	title = {The {Silicon} {Ceiling}: {Auditing} {GPT}'s {Race} and {Gender} {Biases} in {Hiring}},
	shorttitle = {The {Silicon} {Ceiling}},
	url = {http://arxiv.org/abs/2405.04412},
	doi = {10.48550/arXiv.2405.04412},
	abstract = {Large language models (LLMs) are increasingly being introduced in workplace settings, with the goals of improving efficiency and fairness. However, concerns have arisen regarding these models' potential to reflect or exacerbate social biases and stereotypes. This study explores the potential impact of LLMs on hiring practices. To do so, we conduct an algorithm audit of race and gender biases in one commonly-used LLM, OpenAI's GPT-3.5, taking inspiration from the history of traditional offline resume audits. We conduct two studies using names with varied race and gender connotations: resume assessment (Study 1) and resume generation (Study 2). In Study 1, we ask GPT to score resumes with 32 different names (4 names for each combination of the 2 gender and 4 racial groups) and two anonymous options across 10 occupations and 3 evaluation tasks (overall rating, willingness to interview, and hireability). We find that the model reflects some biases based on stereotypes. In Study 2, we prompt GPT to create resumes (10 for each name) for fictitious job candidates. When generating resumes, GPT reveals underlying biases; women's resumes had occupations with less experience, while Asian and Hispanic resumes had immigrant markers, such as non-native English and non-U.S. education and work experiences. Our findings contribute to a growing body of literature on LLM biases, in particular when used in workplace contexts.},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Armstrong, Lena and Liu, Abbey and MacNeil, Stephen and Metaxa, Danaë},
	month = may,
	year = {2024},
	note = {arXiv:2405.04412 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/FB2YMCTH/Armstrong et al. - 2024 - The Silicon Ceiling Auditing GPT's Race and Gende.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/ZBHG5DW5/2405.html:text/html},
}

@article{navigli_biases_2023,
	title = {Biases in {Large} {Language} {Models}: {Origins}, {Inventory}, and {Discussion}},
	volume = {15},
	issn = {1936-1955},
	shorttitle = {Biases in {Large} {Language} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3597307},
	doi = {10.1145/3597307},
	abstract = {In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias.},
	number = {2},
	urldate = {2024-05-16},
	journal = {Journal of Data and Information Quality},
	author = {Navigli, Roberto and Conia, Simone and Ross, Björn},
	month = jun,
	year = {2023},
	keywords = {Bias in NLP, language models},
	pages = {10:1--10:21},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/HYN9QLUT/Navigli et al. - 2023 - Biases in Large Language Models Origins, Inventor.pdf:application/pdf},
}

@misc{gozalo-brizuela_survey_2023,
	title = {A survey of {Generative} {AI} {Applications}},
	url = {http://arxiv.org/abs/2306.02781},
	doi = {10.48550/arXiv.2306.02781},
	abstract = {Generative AI has experienced remarkable growth in recent years, leading to a wide array of applications across diverse domains. In this paper, we present a comprehensive survey of more than 350 generative AI applications, providing a structured taxonomy and concise descriptions of various unimodal and even multimodal generative AIs. The survey is organized into sections, covering a wide range of unimodal generative AI applications such as text, images, video, gaming and brain information. Our survey aims to serve as a valuable resource for researchers and practitioners to navigate the rapidly expanding landscape of generative AI, facilitating a better understanding of the current state-of-the-art and fostering further innovation in the field.},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Gozalo-Brizuela, Roberto and Garrido-Merchán, Eduardo C.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02781 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/LNGTKFA3/Gozalo-Brizuela and Garrido-Merchán - 2023 - A survey of Generative AI Applications.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/NXSICBXP/2306.html:text/html},
}

@article{sison_chatgpt_2023,
	title = {{ChatGPT}: {More} {Than} a “{Weapon} of {Mass} {Deception}” {Ethical} {Challenges} and {Responses} from the {Human}-{Centered} {Artificial} {Intelligence} ({HCAI}) {Perspective}},
	volume = {0},
	issn = {1044-7318},
	shorttitle = {{ChatGPT}},
	url = {https://doi.org/10.1080/10447318.2023.2225931},
	doi = {10.1080/10447318.2023.2225931},
	abstract = {This article explores the ethical problems arising from the use of ChatGPT as a kind of generative AI and suggests responses based on the Human-Centered Artificial Intelligence (HCAI) framework. The HCAI framework is appropriate because it understands technology above all as a tool to empower, augment, and enhance human agency while referring to human wellbeing as a “grand challenge,” thus perfectly aligning itself with ethics, the science of human flourishing. Further, HCAI provides objectives, principles, procedures, and structures for reliable, safe, and trustworthy AI which we apply to our ChatGPT assessments. The main danger ChatGPT presents is the propensity to be used as a “weapon of mass deception” (WMD) and an enabler of criminal activities involving deceit. We review technical specifications to better comprehend its potentials and limitations. We then suggest both technical (watermarking, styleme, detectors, and fact-checkers) and non-technical measures (terms of use, transparency, educator considerations, HITL) to mitigate ChatGPT misuse or abuse and recommend best uses (creative writing, non-creative writing, teaching and learning). We conclude with considerations regarding the role of hu mans in ensuring the proper use of ChatGPT for individual and social wellbeing.},
	number = {0},
	urldate = {2024-05-16},
	journal = {International Journal of Human–Computer Interaction},
	author = {Sison, Alejo José G. and Daza, Marco Tulio and Gozalo-Brizuela, Roberto and Garrido-Merchán, Eduardo C.},
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2023.2225931},
	keywords = {ChatGPT, AI ethics, combating disinformation, generative AI, HCAI},
	pages = {1--20},
	file = {Submitted Version:/Users/daniel/Zotero/storage/ZS3XESJM/Sison et al. - 2023 - ChatGPT More Than a “Weapon of Mass Deception” Et.pdf:application/pdf},
}

@article{crenshaw_demarginalizing_1989,
	title = {Demarginalizing the {Intersection} of {Race} and {Sex}: {A} {Black} {Feminist} {Critique} of {Antidiscrimination} {Doctrine}, {Feminist} {Theory} and {Antiracist} {Politics}},
	volume = {1989},
	shorttitle = {Demarginalizing the {Intersection} of {Race} and {Sex}},
	url = {https://scholarship.law.columbia.edu/faculty_scholarship/3007},
	journal = {U. Chi. Legal F.},
	author = {Crenshaw, Kimberlé},
	month = jan,
	year = {1989},
	pages = {139},
	file = {"Demarginalizing the Intersection of Race and Sex\: A Black Feminist Cri" by Kimberlé W. Crenshaw:/Users/daniel/Zotero/storage/BUMIPWHK/3007.html:text/html;Crenshaw - 1989 - Demarginalizing the Intersection of Race and Sex .pdf:/Users/daniel/Zotero/storage/P88H3UEC/Crenshaw - 1989 - Demarginalizing the Intersection of Race and Sex .pdf:application/pdf},
}

@article{hankivsky_intersectionality_2022,
	title = {{INTERSECTIONALITY} 101},
	url = {https://resources.equityinitiative.org/handle/ei/433},
	abstract = {Interest in and applications of intersectionality have grown exponentially in popularity 
over the last 15 years. Scholars across the globe from a variety of disciplines, including 
sociology, political science, health sciences, geography, philosophy and anthropology, as 
well as in feminist studies, ethnic studies, queer studies and legal studies, have drawn on 
intersectionality to challenge inequities and promote social justice. This practice has also 
extended to policy makers, human rights activists and community organizers search 
- 
ing for better approaches to tackling complex social issues. Yet most people don’t know 
about intersectionality and why it is such an innovative framework for research, policy 
and practice. 
The aim of this primer is to provide a clear-language guide to intersectionality; we 
explore its key elements and characteristics, how it is distinct from other approaches to 
equity, and how it can be applied in research, policy, practice and teaching. Most im 
- 
portantly, the primer aims to show how intersectionality can fundamentally alter how 
social problems are experienced, identified and grasped to include the breadth of lived 
experiences.},
	language = {en},
	urldate = {2024-05-16},
	author = {Hankivsky, Olena},
	month = jul,
	year = {2022},
	note = {Accepted: 2022-07-07T08:13:23Z
Publisher: Institute for Intersectionality Research and Policy},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/IW63FY8A/Hankivsky - 2022 - INTERSECTIONALITY 101.pdf:application/pdf},
}

@article{blank_tracing_2005-2,
	title = {Tracing the {Economic} {Impact} of {Cumulative} {Discrimination}},
	volume = {95},
	issn = {0002-8282},
	url = {https://www.jstor.org/stable/4132798},
	number = {2},
	urldate = {2024-05-17},
	journal = {The American Economic Review},
	author = {Blank, Rebecca M.},
	year = {2005},
	note = {Publisher: American Economic Association},
	pages = {99--103},
	file = {Blank - 2005 - Tracing the Economic Impact of Cumulative Discrimi.pdf:/Users/daniel/Zotero/storage/KESQBP4T/Blank - 2005 - Tracing the Economic Impact of Cumulative Discrimi.pdf:application/pdf},
}

@article{bauer_intersectionality_2021,
	title = {Intersectionality in quantitative research: {A} systematic review of its emergence and applications of theory and methods},
	volume = {14},
	issn = {2352-8273},
	shorttitle = {Intersectionality in quantitative research},
	url = {https://www.sciencedirect.com/science/article/pii/S2352827321000732},
	doi = {10.1016/j.ssmph.2021.100798},
	abstract = {Background
Intersectionality is a theoretical framework rooted in the premise that human experience is jointly shaped by multiple social positions (e.g. race, gender), and cannot be adequately understood by considering social positions independently. Used widely in qualitative studies, its uptake in quantitative research has been more recent.
Objectives
To characterize quantitative research applications of intersectionality from 1989 to mid-2020, to evaluate basic integration of theoretical frameworks, and to identify innovative methods that could be applied to health research.
Methods
Adhering to PRISMA guidelines, we conducted a systematic review of peer-reviewed articles indexed within Scopus, Medline, ProQuest Political Science and Public Administration, and PsycINFO. Original English-language quantitative or mixed-methods research or methods papers that explicitly applied intersectionality theoretical frameworks were included. Experimental studies on perception/stereotyping and measures development or validation studies were excluded. We extracted data related to publication, study design, quantitative methods, and application of intersectionality.
Results
707 articles (671 applied studies, 25 methods-only papers, 11 methods plus application) met inclusion criteria. Articles were published in journals across a range of disciplines, most commonly psychology, sociology, and medical/life sciences; 40.8\% studied a health-related outcome. Results supported concerns among intersectionality scholars that core theoretical tenets are often lost or misinterpreted in quantitative research; about one in four applied articles (26.9\%) failed to define intersectionality, while one in six (17.5\%) included intersectional position components not reflective of social power. Quantitative methods were simplistic (most often regression with interactions, cross-classified variables, or stratification) and were often misapplied or misinterpreted. Several novel methods were identified.
Conclusions
Intersectionality is frequently misunderstood when bridging theory into quantitative methodology. Further work is required to (1) ensure researchers understand key features that define quantitative intersectionality analyses, (2) improve reporting practices for intersectional analyses, and (3) develop and adapt quantitative methods.},
	urldate = {2024-05-17},
	journal = {SSM - Population Health},
	author = {Bauer, Greta R. and Churchill, Siobhan M. and Mahendran, Mayuri and Walwyn, Chantel and Lizotte, Daniel and Villa-Rueda, Alma Angelica},
	month = jun,
	year = {2021},
	keywords = {Systematic review, Statistics, Epidemiology, Intersectionality, Research methods},
	pages = {100798},
	file = {Full Text:/Users/daniel/Zotero/storage/TZSDRARG/Bauer et al. - 2021 - Intersectionality in quantitative research A syst.pdf:application/pdf;ScienceDirect Snapshot:/Users/daniel/Zotero/storage/8N6LNLB2/S2352827321000732.html:text/html},
}

@article{levandowski_we_2024,
	title = {We are complex beings: comparison of statistical methods to capture and account for intersectionality},
	volume = {14},
	copyright = {© Author(s) (or their employer(s)) 2024. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
	issn = {2044-6055, 2044-6055},
	shorttitle = {We are complex beings},
	url = {https://bmjopen.bmj.com/content/14/1/e077194},
	doi = {10.1136/bmjopen-2023-077194},
	abstract = {Objectives Intersectionality conceptualises how different parts of our identity compound, creating unique and multifaceted experiences of oppression. Our objective was to explore and compare several quantitative analytical approaches to measure interactions among four sociodemographic variables and interpret the relative impact of axes of marginalisation on self-reported health, to visualise the potential elevated impact of intersectionality on health outcomes.
Design Secondary analysis of National Epidemiologic Survey on Alcohol and Related Conditions-III, a nationally representative cross-sectional study of 36 309 non-institutionalised US citizens aged 18 years or older.
Primary outcome measures We assessed the effect of interactions among race/ethnicity, disability status, sexual orientation and income level on a self-reported health outcome with three approaches: non-intersectional multivariate regression, intersectional multivariate regression with a single multicategorical predictor variable and intersectional multivariate regression with two-way interactions.
Results Multivariate regression with a single multicategorical predictor variable allows for more flexibility in a logistic regression problem. In the fully fitted model, compared with individuals who were white, above the poverty level, had no disability and were heterosexual (referent), only those who were white, above the poverty level, had no disability and were gay/lesbian/bisexual/not sure (LGBQ+) demonstrated no significant difference in the odds of reporting excellent/very good health (aOR=0.90, 95\% CI=0.71 to 1.13, p=0.36). Multivariate regression with two-way interactions modelled the extent that the relationship between each predictor and outcome depended on the value of a third predictor variable, allowing social position variation at several intersections. For example, compared with heterosexual individuals, LGBQ+ individuals had lower odds of reporting better health among whites (aOR=0.94, 95\% CI=0.93 to 0.95) but higher odds of reporting better health among Black Indigenous People of Color (BIPOC) individuals (aOR=1.13, 95\% CI=1.11 to 1.15).
Conclusion These quantitative approaches help us to understand compounding intersectional experiences within healthcare, to plan interventions and policies that address multiple needs simultaneously.},
	language = {en},
	number = {1},
	urldate = {2024-05-17},
	journal = {BMJ Open},
	author = {Levandowski, Brooke A. and Pro, George C. and Rietberg-Miller, Susan B. and Camplain, Ricky},
	month = jan,
	year = {2024},
	pmid = {38296287},
	note = {Publisher: British Medical Journal Publishing Group
Section: Epidemiology},
	keywords = {Epidemiology, Health Equity, Public health, Sexual and Gender Minorities, STATISTICS \& RESEARCH METHODS},
	pages = {e077194},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/PHFICJFX/Levandowski et al. - 2024 - We are complex beings comparison of statistical m.pdf:application/pdf},
}

@article{hardy_bias_2022,
	title = {Bias in {Context}: {Small} {Biases} in {Hiring} {Evaluations} {Have} {Big} {Consequences}},
	volume = {48},
	issn = {0149-2063},
	shorttitle = {Bias in {Context}},
	url = {https://doi.org/10.1177/0149206320982654},
	doi = {10.1177/0149206320982654},
	abstract = {It is widely acknowledged that subgroup bias can influence hiring evaluations. However, the notion that bias still threatens equitable hiring outcomes in modern employment contexts continues to be debated, even among organizational scholars. In this study, we sought to contextualize this debate by estimating the practical impact of bias on real-world hiring outcomes (a) across a wide range of hiring scenarios and (b) in the presence of diversity-oriented staffing practices. Toward this end, we conducted a targeted meta-analysis of recent hiring experiments that manipulated both candidate gender and qualifications to couch our investigation within ongoing debates surrounding the impact of small amounts of bias in otherwise meritocratic hiring contexts. Consistent with prior research, we found evidence of small gender bias effects (d = −0.30) and large qualification effects (d = 1.61) on hiring managers’ evaluations of candidate hireability. We then used these values to inform the starting parameters of a large-scale computer simulation designed to model conventional processes by which candidates are recruited, evaluated, and selected for open positions. Collectively, our simulation findings empirically substantiate assertions that even seemingly trivial amounts of subgroup bias can produce practically significant rates of hiring discrimination and productivity loss. Furthermore, we found contextual factors can alter but cannot obviate the consequences of biased evaluations, even within apparently optimal hiring scenarios (e.g., when extremely valid assessments are used). Finally, our results demonstrate residual amounts of subgroup bias can undermine the effectiveness of otherwise successful targeted recruitment efforts. Implications for future research and practice are discussed.},
	language = {en},
	number = {3},
	urldate = {2024-05-17},
	journal = {Journal of Management},
	author = {Hardy, Jay H. and Tey, Kian Siong and Cyrus-Lai, Wilson and Martell, Richard F. and Olstad, Andy and Uhlmann, Eric Luis},
	month = mar,
	year = {2022},
	note = {Publisher: SAGE Publications Inc},
	pages = {657--692},
}

@inproceedings{buolamwini_gender_2018,
	title = {Gender {Shades}: {Intersectional} {Accuracy} {Disparities} in {Commercial} {Gender} {Classification}},
	shorttitle = {Gender {Shades}},
	url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
	abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
	language = {en},
	urldate = {2024-05-18},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Buolamwini, Joy and Gebru, Timnit},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {77--91},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/YEBBCRR7/Buolamwini and Gebru - 2018 - Gender Shades Intersectional Accuracy Disparities.pdf:application/pdf;Supplementary PDF:/Users/daniel/Zotero/storage/TH9MVYIB/Buolamwini and Gebru - 2018 - Gender Shades Intersectional Accuracy Disparities.pdf:application/pdf},
}

@misc{bisbee_synthetic_2023,
	title = {Synthetic {Replacements} for {Human} {Survey} {Data}? {The} {Perils} of {Large} {Language} {Models}},
	shorttitle = {Synthetic {Replacements} for {Human} {Survey} {Data}?},
	url = {https://osf.io/5ecfa},
	doi = {10.31235/osf.io/5ecfa},
	abstract = {Large Language Models (LLMs) offer new research possibilities for social scientists, but their potential as "synthetic data" is still largely unknown. In this paper, we investigate how accurately the popular closed-source LLM ChatGPT can recover public opinion, prompting the LLM to adopt different "personas" and then provide feeling thermometer scores for 11 sociopolitical groups. The average scores generated by ChatGPT correspond closely to the averages in our baseline survey, the 2016–2020 American National Election Study. Nevertheless, sampling by ChatGPT is not reliable for statistical inference: there is less variation in responses than in the real surveys, and regression coefficients often differ significantly from equivalent estimates obtained using ANES data. We also document how the distribution of synthetic responses varies with minor changes in prompt wording, and we show how the same prompt yields significantly different results over a three-month period. Altogether, our findings raise serious concerns about the quality, reliability, and reproducibility of synthetic survey data generated by LLMs.},
	language = {en-us},
	urldate = {2024-05-18},
	publisher = {OSF},
	author = {Bisbee, James and Clinton, Joshua and Dorff, Cassy and Kenkel, Brenton and Larson, Jennifer},
	month = may,
	year = {2023},
	file = {Submitted Version:/Users/daniel/Zotero/storage/B4RFJZ3L/Bisbee et al. - 2023 - Synthetic Replacements for Human Survey Data The .pdf:application/pdf},
}

@article{lippens_computer_2024,
	title = {Computer says ‘no’: {Exploring} systemic bias in {ChatGPT} using an audit approach},
	volume = {2},
	issn = {2949-8821},
	shorttitle = {Computer says ‘no’},
	url = {https://www.sciencedirect.com/science/article/pii/S2949882124000148},
	doi = {10.1016/j.chbah.2024.100054},
	abstract = {Large language models offer significant potential for increasing labour productivity, such as streamlining personnel selection, but raise concerns about perpetuating systemic biases embedded into their pre-training data. This study explores the potential ethnic and gender bias of ChatGPT—a chatbot producing human-like responses to language tasks—in assessing job applicants. Using the correspondence audit approach from the social sciences, I simulated a CV screening task with 34,560 vacancy–CV combinations where the chatbot had to rate fictitious applicant profiles. Comparing ChatGPT's ratings of Arab, Asian, Black American, Central African, Dutch, Eastern European, Hispanic, Turkish, and White American male and female applicants, I show that ethnic and gender identity influence the chatbot's evaluations. Ethnic discrimination is more pronounced than gender discrimination and mainly occurs in jobs with favourable labour conditions or requiring greater language proficiency. In contrast, gender bias emerges in gender-atypical roles. These findings suggest that ChatGPT's discriminatory output reflects a statistical mechanism echoing societal stereotypes. Policymakers and developers should address systemic bias in language model-driven applications to ensure equitable treatment across demographic groups. Practitioners should practice caution, given the adverse impact these tools can (re)produce, especially in selection decisions involving humans.},
	number = {1},
	urldate = {2024-05-21},
	journal = {Computers in Human Behavior: Artificial Humans},
	author = {Lippens, Louis},
	month = jan,
	year = {2024},
	keywords = {ChatGPT, Correspondence audit, Hiring discrimination, Large language models, Systemic bias},
	pages = {100054},
	file = {Lippens - 2024 - Computer says ‘no’ Exploring systemic bias in Cha.pdf:/Users/daniel/Zotero/storage/MSIKTT87/Lippens - 2024 - Computer says ‘no’ Exploring systemic bias in Cha.pdf:application/pdf;ScienceDirect Snapshot:/Users/daniel/Zotero/storage/R77TKG4V/S2949882124000148.html:text/html},
}

@article{miller_explanation_2019,
	title = {Explanation in artificial intelligence: {Insights} from the social sciences},
	volume = {267},
	issn = {0004-3702},
	shorttitle = {Explanation in artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
	doi = {10.1016/j.artint.2018.07.007},
	abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	urldate = {2024-05-29},
	journal = {Artificial Intelligence},
	author = {Miller, Tim},
	month = feb,
	year = {2019},
	keywords = {Explainability, Explainable AI, Explanation, Interpretability, Transparency},
	pages = {1--38},
	file = {ScienceDirect Snapshot:/Users/daniel/Zotero/storage/ZVAARVKX/S0004370218305988.html:text/html;Submitted Version:/Users/daniel/Zotero/storage/5I7DWCWP/Miller - 2019 - Explanation in artificial intelligence Insights f.pdf:application/pdf},
}

@article{haque_exploring_2024,
	title = {Exploring {ChatGPT} and its impact on society},
	issn = {2730-5961},
	url = {https://doi.org/10.1007/s43681-024-00435-4},
	doi = {10.1007/s43681-024-00435-4},
	abstract = {Artificial intelligence has been around for a while, but suddenly it has received more attention than ever before. Thanks to innovations from companies like Google, Microsoft, Meta, and other major brands in technology. OpenAI, though, has triggered the button with its ground-breaking invention—“ChatGPT”. ChatGPT is a Large Language Model (LLM) based on Transformer architecture that has the ability to generate human-like responses in a conversational context. It uses deep learning algorithms to generate natural language responses to input text. Its large number of parameters, contextual generation, and open-domain training make it a versatile and effective tool for a wide range of applications, from chatbots to customer service to language translation. It has the potential to revolutionize various industries and transform the way we interact with technology. However, the use of ChatGPT has also raised several concerns, including ethical, social, and employment challenges, which must be carefully considered to ensure the responsible use of this technology. The article provides an overview of ChatGPT, delving into its architecture and training process. It highlights the potential impacts of ChatGPT on the society. In this paper, we suggest some approaches involving technology, regulation, education, and ethics in an effort to maximize ChatGPT’s benefits while minimizing its negative impacts. This study is expected to contribute to a greater understanding of ChatGPT and aid in predicting the potential changes it may bring about.},
	language = {en},
	urldate = {2024-05-29},
	journal = {AI and Ethics},
	author = {Haque, Md. Asraful and Li, Shuai},
	month = feb,
	year = {2024},
	keywords = {ChatGPT, Generative AI, LLM, OpenAI, RLHF},
	file = {Haque and Li - 2024 - Exploring ChatGPT and its impact on society.pdf:/Users/daniel/Zotero/storage/78BVHV29/Haque and Li - 2024 - Exploring ChatGPT and its impact on society.pdf:application/pdf},
}

@article{joyce_toward_2021,
	title = {Toward a {Sociology} of {Artificial} {Intelligence}: {A} {Call} for {Research} on {Inequalities} and {Structural} {Change}},
	volume = {7},
	issn = {2378-0231},
	shorttitle = {Toward a {Sociology} of {Artificial} {Intelligence}},
	url = {https://doi.org/10.1177/2378023121999581},
	doi = {10.1177/2378023121999581},
	abstract = {This article outlines a research agenda for a sociology of artificial intelligence (AI). The authors review two areas in which sociological theories and methods have made significant contributions to the study of inequalities and AI: (1) the politics of algorithms, data, and code and (2) the social shaping of AI in practice. The authors contrast sociological approaches that emphasize intersectional inequalities and social structure with other disciplines’ approaches to the social dimensions of AI, which often have a thin understanding of the social and emphasize individual-level interventions. This scoping article invites sociologists to use the discipline’s theoretical and methodological tools to analyze when and how inequalities are made more durable by AI systems. Sociologists have an ability to identify how inequalities are embedded in all aspects of society and to point toward avenues for structural social change. Therefore, sociologists should play a leading role in the imagining and shaping of AI futures.},
	language = {en},
	urldate = {2024-06-26},
	journal = {Socius},
	author = {Joyce, Kelly and Smith-Doerr, Laurel and Alegria, Sharla and Bell, Susan and Cruz, Taylor and Hoffman, Steve G. and Noble, Safiya Umoja and Shestakofsky, Benjamin},
	month = jan,
	year = {2021},
	note = {Publisher: SAGE Publications},
	pages = {2378023121999581},
	file = {SAGE PDF Full Text:/Users/daniel/Zotero/storage/NE74258A/Joyce et al. - 2021 - Toward a Sociology of Artificial Intelligence A C.pdf:application/pdf},
}

@article{gomez-cruz_descolonizando_2023,
	title = {Descolonizando los métodos para estudiar la cultura digital: una propuesta desde {Latinoamérica}},
	copyright = {Derechos de autor 2023 Cuadernos.info},
	issn = {0719-367X},
	shorttitle = {Descolonizando los métodos para estudiar la cultura digital},
	url = {https://cuadernos.info/index.php/cdi/article/view/52605},
	doi = {10.7764/cdi.54.52605},
	abstract = {This paper makes an epistemic political intervention in three parts. First, we elaborate a critique of the theoretical methodological logics usually reproduced without considering the differences between the place where the methods originated and the place where they are applied. Specifically, we problematize the idea of novelty, which has been predominant in the study of digital phenomena. Second, we discuss some elements of the so-called decolonial turn that we consider inspiring to account for the relationship between methodologies and research on digital culture. Third, we advance a series of specific proposals to develop methodologies that respond to the specific contexts of Latin America and digital culture.},
	language = {es},
	number = {54},
	urldate = {2024-07-19},
	journal = {Cuadernos.info},
	author = {Gómez-Cruz, Edgar and Ricaurte, Paola and Siles, Ignacio},
	month = jan,
	year = {2023},
	note = {Number: 54},
	keywords = {cultura digital},
	pages = {160--181},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/Y3P5ARTY/Gómez-Cruz et al. - 2023 - Descolonizando los métodos para estudiar la cultur.pdf:application/pdf},
}

@article{gallistl_vulnerability_2024,
	title = {Vulnerability {Assemblages}: {Situating} {Vulnerability} in the {Political} {Economy} of {Artificial} {Intelligence}},
	volume = {10},
	issn = {2378-0231},
	shorttitle = {Vulnerability {Assemblages}},
	url = {https://doi.org/10.1177/23780231241266514},
	doi = {10.1177/23780231241266514},
	abstract = {“Vulnerability” is one of the terms recently used to discuss ethical aspects of artificial intelligence (AI). Current discussions on AI vulnerability tend to individualize vulnerability, largely neglecting its political dimensions, which are rooted in systems of inequality and disadvantage. This article draws on data from a multiple-perspective qualitative interview study to explore how notions of vulnerability underpin the development and implementation of AI. Results uncover how AI designers use narratives around missing data on vulnerable populations as justifications for the creation of synthetic data that were artificially manufactured rather than generated by real-world events. Although this was a profitable business model for AI companies, these practices ultimately situated long-term care residents as voiceless in the development of AI. This contribution shows how vulnerability is situated in a political economy of AI, which understands the absence of data on vulnerable groups as a possibility of profit creation rather than a chance of fostering inclusion.},
	language = {en},
	urldate = {2024-07-29},
	journal = {Socius},
	author = {Gallistl, Vera and von Laufenberg, Roger and Lehner, Katrin},
	month = jan,
	year = {2024},
	note = {Publisher: SAGE Publications},
	pages = {23780231241266514},
	file = {SAGE PDF Full Text:/Users/daniel/Zotero/storage/Z6ID6VJZ/Gallistl et al. - 2024 - Vulnerability Assemblages Situating Vulnerability.pdf:application/pdf},
}

@article{law_artificial_2024,
	title = {Artificial {Intelligence} {Policymaking}: {An} {Agenda} for {Sociological} {Research}},
	volume = {10},
	issn = {2378-0231},
	shorttitle = {Artificial {Intelligence} {Policymaking}},
	url = {https://doi.org/10.1177/23780231241261596},
	doi = {10.1177/23780231241261596},
	abstract = {Sociological research on artificial intelligence (AI) is flourishing: sociologists of inequality are examining new and concerning effects of AI on American society, and computational sociologists are developing novel ways to use AI in research. The authors advocate for a third form of sociological engagement with AI: research on how AI can be publicly governed to advance equity in American society. The authors orient sociologists to the rapidly evolving AI policy landscape by defining AI and by contrasting two leading approaches to AI governance: a safety-based and an equity-based approach. The authors argue that the safety-based approach is the predominant one but is inadequate. They suggest that sociologists can shift AI policymaking to prioritize equity by examining corporate political power in AI policy debates and by organizing research around four sociological questions centered on equity and public engagement. The authors conclude with recommendations for supporting and coordinating policy-oriented research on AI in sociology.},
	language = {en},
	urldate = {2024-08-01},
	journal = {Socius},
	author = {Law, Tina and McCall, Leslie},
	month = jan,
	year = {2024},
	note = {Publisher: SAGE Publications},
	pages = {23780231241261596},
	file = {SAGE PDF Full Text:/Users/daniel/Zotero/storage/B6NSGQHD/Law and McCall - 2024 - Artificial Intelligence Policymaking An Agenda fo.pdf:application/pdf},
}

@misc{ayyamperumal_current_2024,
	title = {Current state of {LLM} {Risks} and {AI} {Guardrails}},
	url = {http://arxiv.org/abs/2406.12934},
	doi = {10.48550/arXiv.2406.12934},
	abstract = {Large language models (LLMs) have become increasingly sophisticated, leading to widespread deployment in sensitive applications where safety and reliability are paramount. However, LLMs have inherent risks accompanying them, including bias, potential for unsafe actions, dataset poisoning, lack of explainability, hallucinations, and non-reproducibility. These risks necessitate the development of "guardrails" to align LLMs with desired behaviors and mitigate potential harm. This work explores the risks associated with deploying LLMs and evaluates current approaches to implementing guardrails and model alignment techniques. We examine intrinsic and extrinsic bias evaluation methods and discuss the importance of fairness metrics for responsible AI development. The safety and reliability of agentic LLMs (those capable of real-world actions) are explored, emphasizing the need for testability, fail-safes, and situational awareness. Technical strategies for securing LLMs are presented, including a layered protection model operating at external, secondary, and internal levels. System prompts, Retrieval-Augmented Generation (RAG) architectures, and techniques to minimize bias and protect privacy are highlighted. Effective guardrail design requires a deep understanding of the LLM's intended use case, relevant regulations, and ethical considerations. Striking a balance between competing requirements, such as accuracy and privacy, remains an ongoing challenge. This work underscores the importance of continuous research and development to ensure the safe and responsible use of LLMs in real-world applications.},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Ayyamperumal, Suriya Ganesh and Ge, Limin},
	month = jun,
	year = {2024},
	note = {arXiv:2406.12934 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/C67VEJJG/Ayyamperumal and Ge - 2024 - Current state of LLM Risks and AI Guardrails.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/LX68QEQU/2406.html:text/html},
}

@article{misra_methods_2021,
	title = {Methods of intersectional research},
	volume = {41},
	issn = {0273-2173},
	url = {https://doi.org/10.1080/02732173.2020.1791772},
	doi = {10.1080/02732173.2020.1791772},
	abstract = {Intersectionality is a powerful concept within sociology, urging scholars to consider how an array of socially constructed dimensions of difference intersect to shape each person’s experiences and actions. This paper provides a number of different blueprints for designing intersectional research, which can be adapted for different purposes. The key methodological tenets of intersectional research are oppression, relationality, complexity, context, comparison, and deconstruction. This paper defines these tenets, addresses misunderstandings of their implications, and applies these tenets to existing intersectional research. Multiple qualitative, comparative, and quantitative strategies can be used to carry out intersectional research; there is not just one way to do intersectional empirical research. While intersectional methods require thought in designing the research, they are doable. What is more, they provide much more nuanced understandings of social relations and inequality. If race, class, gender and other socially constructed dimensions of difference are understood not as static but as dynamic, researchers can employ a wide variety of methodological tools to analyze power relations via their intersections.},
	number = {1},
	urldate = {2024-08-09},
	journal = {Sociological Spectrum},
	author = {Misra, Joya and Curington, Celeste Vaughan and Green, Venus Mary},
	month = jan,
	year = {2021},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/02732173.2020.1791772},
	pages = {9--28},
}

@article{ekuma_artificial_2024,
	title = {Artificial {Intelligence} and {Automation} in {Human} {Resource} {Development}: {A} {Systematic} {Review}},
	volume = {23},
	issn = {1534-4843},
	shorttitle = {Artificial {Intelligence} and {Automation} in {Human} {Resource} {Development}},
	url = {https://doi.org/10.1177/15344843231224009},
	doi = {10.1177/15344843231224009},
	abstract = {This systematic review synthesizes the existing literature on the impact of artificial intelligence (AI) and automation on Human Resource Development (HRD) practices and outcomes. The study explores how AI and automation affect HRD, highlighting specific HRD processes affected and their influence on outcomes. A comprehensive search was conducted across academic databases, HRD journals, and conference proceedings, resulting in a selection of relevant studies. The findings were analyzed through a narrative synthesis, with subgroup analyses based on specific HRD processes. The review provides insights into AI and automation implications for HRD researchers and practitioners. It also identifies research gaps and future directions.},
	language = {en},
	number = {2},
	urldate = {2024-08-20},
	journal = {Human Resource Development Review},
	author = {Ekuma, Kelechi},
	month = jun,
	year = {2024},
	note = {Publisher: SAGE Publications},
	pages = {199--229},
	file = {SAGE PDF Full Text:/Users/daniel/Zotero/storage/K778N9N3/Ekuma - 2024 - Artificial Intelligence and Automation in Human Re.pdf:application/pdf},
}

@misc{gaddis_discrimination_2021,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Discrimination {Against} {Black} and {Hispanic} {Americans} is {Highest} in {Hiring} and {Housing} {Contexts}: {A} {Meta}-{Analysis} of {Correspondence} {Audits}},
	shorttitle = {Discrimination {Against} {Black} and {Hispanic} {Americans} is {Highest} in {Hiring} and {Housing} {Contexts}},
	url = {https://papers.ssrn.com/abstract=3975770},
	doi = {10.2139/ssrn.3975770},
	abstract = {To what extent does racial/ethnic discrimination in America differ across contexts? In this paper, we provide the largest and most comprehensive review of racial/ethnic discrimination research to date. We conducted a meta-analysis of 78 correspondence audits in the United States, representing over half a million applications, emails, and other forms of correspondence that occur in all aspects of modern society, including the hiring, housing, medical, public services, and education sectors. We find that racial/ethnic discrimination in the United States continues to be a large problem, but discrimination against racial/ethnic minorities simultaneously exhibits a substantial amount of contextual heterogeneity not recognized in previous discrimination research. Discrimination against Black Americans is most common in hiring, followed by the rental housing context. Discrimination against Hispanic Americans is highest in hiring, but discrimination in other contexts is considerably lower. Although discrimination occurs in education, medical, and public services contexts, it is far less common in these sectors. Altogether, our findings suggest that discrimination is more common in economic contexts that are more resource-intensive and have higher stakes, despite stronger legal protections against discrimination in those same contexts. Our work confirms that racial/ethnic discrimination in the United States continues to be a persistent and pervasive phenomenon that impacts many core parts of the lives of Black and Hispanic Americans and simultaneously reinforces and exacerbates existing inequalities.},
	language = {en},
	urldate = {2024-08-20},
	author = {Gaddis, S. Michael and Larsen, Edvard and Crabtree, Charles and Holbein, John},
	month = dec,
	year = {2021},
	keywords = {field experiment, correspondence audit, meta-analysis, racial/ethnic discrimination},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/GRYHU97C/Gaddis et al. - 2021 - Discrimination Against Black and Hispanic American.pdf:application/pdf},
}

@incollection{chan_artificial_2022,
	address = {Cham},
	title = {Artificial {Intelligence} in {Real} {Estate}},
	isbn = {978-3-031-05740-3},
	url = {https://doi.org/10.1007/978-3-031-05740-3_16},
	abstract = {This chapter explores AI technologies like machine learning, artificial neural networks, predicting algorithms, and others in real estate. It covers AI-powered real estate platforms like Houzen, Homesnap, and NeighborhoodScout. Three case studies are presented in this chapter. The first case analyzes Zillow’s incorporation of machine learning algorithms, data processing, and mining tools to gain competitive advantage in the real estate industry. The second case is about Redfin and its use of AI technologies that assists buyers in the home-searching process. The third case is about Compass and the use of analytical marketing and AI-powered recommendation platforms in the real estate industry.},
	language = {en},
	urldate = {2024-08-20},
	booktitle = {Applied {Artificial} {Intelligence} in {Business}: {Concepts} and {Cases}},
	publisher = {Springer International Publishing},
	author = {Chan, Leong and Hogaboam, Liliya and Cao, Renzhi},
	editor = {Chan, Leong and Hogaboam, Liliya and Cao, Renzhi},
	year = {2022},
	doi = {10.1007/978-3-031-05740-3_16},
	pages = {249--263},
}

@misc{acemoglu_simple_2024,
	type = {Working {Paper}},
	series = {Working {Paper} {Series}},
	title = {The {Simple} {Macroeconomics} of {AI}},
	url = {https://www.nber.org/papers/w32487},
	doi = {10.3386/w32487},
	abstract = {This paper evaluates claims about large macroeconomic implications of new advances in AI. It starts from a task-based model of AI’s effects, working through automation and task complementarities. So long as AI’s microeconomic effects are driven by cost savings/productivity improvements at the task level, its macroeconomic consequences will be given by a version of Hulten’s theorem: GDP and aggregate productivity gains can be estimated by what fraction of tasks are impacted and average task-level cost savings. Using existing estimates on exposure to AI and productivity improvements at the task level, these macroeconomic effects appear nontrivial but modest—no more than a 0.66\% increase in total factor productivity (TFP) over 10 years. The paper then argues that even these estimates could be exaggerated, because early evidence is from easy-to-learn tasks, whereas some of the future effects will come from hard-to-learn tasks, where there are many context-dependent factors affecting decision-making and no objective outcome measures from which to learn successful performance. Consequently, predicted TFP gains over the next 10 years are even more modest and are predicted to be less than 0.53\%. I also explore AI’s wage and inequality effects. I show theoretically that even when AI improves the productivity of low-skill workers in certain tasks (without creating new tasks for them), this may increase rather than reduce inequality. Empirically, I find that AI advances are unlikely to increase inequality as much as previous automation technologies because their impact is more equally distributed across demographic groups, but there is also no evidence that AI will reduce labor income inequality. Instead, AI is predicted to widen the gap between capital and labor income. Finally, some of the new tasks created by AI may have negative social value (such as design of algorithms for online manipulation), and I discuss how to incorporate the macroeconomic effects of new tasks that may have negative social value.},
	urldate = {2024-08-25},
	publisher = {National Bureau of Economic Research},
	author = {Acemoglu, Daron},
	month = may,
	year = {2024},
	doi = {10.3386/w32487},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/FNBUCJ84/Acemoglu - 2024 - The Simple Macroeconomics of AI.pdf:application/pdf},
}

@article{hofmann_ai_2024,
	title = {{AI} generates covertly racist decisions about people based on their dialect},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07856-5},
	doi = {10.1038/s41586-024-07856-5},
	abstract = {Hundreds of millions of people now interact with language models, with uses ranging from help with writing1,2 to informing hiring decisions3. However, these language models are known to perpetuate systematic racial prejudices, making their judgements biased in problematic ways about groups such as African Americans4–7. Although previous research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time, particularly in the United States after the civil rights movement8,9. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice, exhibiting raciolinguistic stereotypes about speakers of African American English (AAE) that are more negative than any human stereotypes about African Americans ever experimentally recorded. By contrast, the language models’ overt stereotypes about African Americans are more positive. Dialect prejudice has the potential for harmful consequences: language models are more likely to suggest that speakers of AAE be assigned less-prestigious jobs, be convicted of crimes and be sentenced to death. Finally, we show that current practices of alleviating racial bias in language models, such as human preference alignment, exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level. Our findings have far-reaching implications for the fair and safe use of language technology.},
	language = {en},
	urldate = {2024-09-01},
	journal = {Nature},
	author = {Hofmann, Valentin and Kalluri, Pratyusha Ria and Jurafsky, Dan and King, Sharese},
	month = aug,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Society},
	pages = {1--8},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/7IDKYHDK/Hofmann et al. - 2024 - AI generates covertly racist decisions about peopl.pdf:application/pdf},
}

@misc{nangia_crows-pairs_2020,
	title = {{CrowS}-{Pairs}: {A} {Challenge} {Dataset} for {Measuring} {Social} {Biases} in {Masked} {Language} {Models}},
	shorttitle = {{CrowS}-{Pairs}},
	url = {http://arxiv.org/abs/2010.00133},
	doi = {10.48550/arXiv.2010.00133},
	abstract = {Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.},
	urldate = {2024-09-01},
	publisher = {arXiv},
	author = {Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R.},
	month = sep,
	year = {2020},
	note = {arXiv:2010.00133 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/daniel/Zotero/storage/33DAZXX9/Nangia et al. - 2020 - CrowS-Pairs A Challenge Dataset for Measuring Soc.pdf:application/pdf;arXiv.org Snapshot:/Users/daniel/Zotero/storage/XHE75JB3/2010.html:text/html},
}

@article{lancee_ethnic_2021,
	title = {Ethnic discrimination in hiring: comparing groups across contexts. {Results} from a cross-national field experiment},
	volume = {47},
	issn = {1369-183X},
	shorttitle = {Ethnic discrimination in hiring},
	url = {https://doi.org/10.1080/1369183X.2019.1622744},
	doi = {10.1080/1369183X.2019.1622744},
	abstract = {Existing field experimental research unequivocally shows the existence of ethnic discrimination in the labour market. Furthermore, studies have documented considerable variation in discrimination rates across countries. However, while the field of discrimination research is rapidly expanding, there are at present no harmonised comparative studies. This is unfortunate, as we do not know why there are cross-national differences in discrimination. In this paper, I present the GEMM study (N = 19,181), a harmonised cross-national field experiment on hiring discrimination. The GEMM study contains 53 ethnic minority groups and is carried out in six countries: Germany, Norway, The Netherlands, Spain, United Kingdom and the United States. Furthermore, I discuss the need and potential for a comparative analysis of discrimination and outline the methodological challenges of carrying out a cross-national field experiment. The special issue presents results for the major ethnic minority groups in six countries and compares discrimination rates across national contexts.},
	number = {6},
	urldate = {2024-09-17},
	journal = {Journal of Ethnic and Migration Studies},
	author = {Lancee, Bram},
	month = apr,
	year = {2021},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/1369183X.2019.1622744},
	keywords = {Ethnic discrimination, field experiments, labour market, comparative research, correspondence tests, employer behaviour},
	pages = {1181--1200},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/XDVDBPTR/Lancee - 2021 - Ethnic discrimination in hiring comparing groups .pdf:application/pdf},
}

@article{zhou_larger_2024,
	title = {Larger and more instructable language models become less reliable},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07930-y},
	doi = {10.1038/s41586-024-07930-y},
	abstract = {The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources1) and bespoke shaping up (including post-filtering2,3, fine tuning or use of human feedback4,5). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, here we show that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. We also find that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, we observe that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount.},
	language = {en},
	urldate = {2024-10-01},
	journal = {Nature},
	author = {Zhou, Lexin and Schellaert, Wout and Martínez-Plumed, Fernando and Moros-Daval, Yael and Ferri, Cèsar and Hernández-Orallo, José},
	month = sep,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Information technology, Computer science, Technology},
	pages = {1--8},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/6WUEYG7J/Zhou et al. - 2024 - Larger and more instructable language models becom.pdf:application/pdf},
}

@article{quillian_countries_2019,
	title = {Do {Some} {Countries} {Discriminate} {More} than {Others}? {Evidence} from 97 {Field} {Experiments} of {Racial} {Discrimination} in {Hiring}},
	volume = {6},
	issn = {2330-6696},
	shorttitle = {Do {Some} {Countries} {Discriminate} {More} than {Others}?},
	url = {https://sociologicalscience.com/articles-v6-18-467/},
	doi = {10.15195/v6.a18},
	abstract = {Comparing levels of discrimination across countries can provide a window into large-scale social and political factors often described as the root of discrimination. Because of difficulties in measurement, however, little is established about variation in hiring discrimination across countries. We address this gap through a formal meta-analysis of 97 field experiments of discrimination incorporating more than 200,000 job applications in nine countries in Europe and North America. We find significant discrimination against nonwhite natives in all countries in our analysis; discrimination against white immigrants is present but low. However, discrimination rates vary strongly by country: In high-discrimination countries, white natives receive nearly twice the callbacks of nonwhites; in low-discrimination countries, white natives receive about 25 percent more. France has the highest discrimination rates, followed by Sweden. We find smaller differences among Great Britain, Canada, Belgium, the Netherlands, Norway, the United States, and Germany. These findings challenge several conventional macro-level theories of discrimination.},
	language = {en-US},
	urldate = {2024-10-02},
	journal = {Sociological Science},
	author = {Quillian, Lincoln and Heath, Anthony and Pager, Devah and Midtbøen, Arnfinn H. and Fleischmann, Fenella and Hexel, Ole},
	month = jun,
	year = {2019},
	pages = {467--496},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/D9FECE9N/Quillian et al. - 2019 - Do Some Countries Discriminate More than Others E.pdf:application/pdf},
}

@book{gaddis_audit_2018,
	address = {New York, NY},
	title = {Audit studies: behind the scenes with theory, method, and nuance},
	isbn = {978-3-319-71152-2},
	shorttitle = {Audit studies},
	publisher = {Springer Berlin Heidelberg},
	author = {Gaddis, S. Michael},
	year = {2018},
}

@article{bertrand_are_2004,
	title = {Are {Emily} and {Greg} {More} {Employable} {Than} {Lakisha} and {Jamal}? {A} {Field} {Experiment} on {Labor} {Market} {Discrimination}},
	volume = {94},
	issn = {0002-8282},
	shorttitle = {Are {Emily} and {Greg} {More} {Employable} {Than} {Lakisha} and {Jamal}?},
	url = {https://pubs.aeaweb.org/doi/10.1257/0002828042002561},
	doi = {10.1257/0002828042002561},
	abstract = {We study race in the labor market by sending fictitious resumes to help-wanted ads in Boston and Chicago newspapers. To manipulate perceived race, resumes are randomly assigned African-American- or White-sounding names. White names receive 50 percent more callbacks for interviews. Callbacks are also more responsive to resume quality for White names than for African-American ones. The racial gap is uniform across occupation, industry, and employer size. We also find little evidence that employers are inferring social class from the names. Differential treatment by race still appears to still be prominent in the U.S. labor market.},
	language = {en},
	number = {4},
	urldate = {2024-10-02},
	journal = {American Economic Review},
	author = {Bertrand, Marianne and Mullainathan, Sendhil},
	month = sep,
	year = {2004},
	pages = {991--1013},
	file = {Full Text:/Users/daniel/Zotero/storage/93PDI3JL/Bertrand and Mullainathan - 2004 - Are Emily and Greg More Employable Than Lakisha an.pdf:application/pdf},
}

@article{tully_express_2025,
	title = {{EXPRESS}: {Lower} {Artificial} {Intelligence} {Literacy} {Predicts} {Greater} {AI} {Receptivity}},
	issn = {0022-2429, 1547-7185},
	shorttitle = {{EXPRESS}},
	url = {https://journals.sagepub.com/doi/10.1177/00222429251314491},
	doi = {10.1177/00222429251314491},
	abstract = {As artificial intelligence (AI) transforms society, understanding factors that influence AI receptivity is increasingly important. The current research investigates which types of consumers have greater AI receptivity. Contrary to expectations revealed in four surveys, cross country data and six additional studies find that people with lower AI literacy are typically more receptive to AI. This lower literacy-greater receptivity link is not explained by differences in perceptions of AI’s capability, ethicality, or feared impact on humanity. Instead, this link occurs because people with lower AI literacy are more likely to perceive AI as magical and experience feelings of awe in the face of AI’s execution of tasks that seem to require uniquely human attributes. In line with this theorizing, the lower literacy-higher receptivity link is mediated by perceptions of AI as magical and is moderated among tasks not assumed to require distinctly human attributes. These findings suggest that companies may benefit from shifting their marketing efforts and product development towards consumers with lower AI literacy. Additionally, efforts to demystify AI may inadvertently reduce its appeal, indicating that maintaining an aura of magic around AI could be beneficial for adoption.},
	language = {en},
	urldate = {2025-01-31},
	journal = {Journal of Marketing},
	author = {Tully, Stephanie and Longoni, Chiara and Appel, Gil},
	month = jan,
	year = {2025},
	pages = {00222429251314491},
}

@article{bommasani_trustworthy_2024,
	title = {Trustworthy {Social} {Bias} {Measurement}},
	volume = {7},
	issn = {3065-8365},
	url = {https://ojs.aaai.org/index.php/AIES/article/view/31630},
	doi = {10.1609/aies.v7i1.31630},
	abstract = {How do we design measures of social bias that we trust?
While prior work has introduced several measures, no measure has gained widespread trust: instead, mounting evidence argues we should distrust these measures. In this work, we design bias measures that warrant trust based on the cross-disciplinary theory of measurement modeling. To combat the frequently fuzzy treatment of social bias in natural language processing, we explicitly define social bias, grounded in principles drawn from social science research. We operationalize our definition by proposing a general bias measurement framework DivDist, which we use to instantiate 5 concrete bias measures. To validate our measures, we propose a rigorous testing protocol with 8 testing criteria (e.g. predictive validity: do measures predict biases in US employment?). Through our testing, we demonstrate considerable evidence to trust our measures, showing they overcome conceptual, technical, and empirical deficiencies present in prior measures.},
	urldate = {2025-02-02},
	journal = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
	author = {Bommasani, Rishi and Liang, Percy},
	month = oct,
	year = {2024},
	pages = {210--224},
	file = {Submitted Version:/Users/daniel/Zotero/storage/DQQJNYW2/Bommasani and Liang - 2024 - Trustworthy Social Bias Measurement.pdf:application/pdf},
}

@misc{fleisig_linguistic_2024,
	title = {Linguistic {Bias} in {ChatGPT}: {Language} {Models} {Reinforce} {Dialect} {Discrimination}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Linguistic {Bias} in {ChatGPT}},
	url = {https://arxiv.org/abs/2406.08818},
	doi = {10.48550/ARXIV.2406.08818},
	abstract = {We present a large-scale study of linguistic bias exhibited by ChatGPT covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-"standard" varieties from around the world). We prompted GPT-3.5 Turbo and GPT-4 with text by native speakers of each variety and analyzed the responses via detailed linguistic feature annotation and native speaker evaluation. We find that the models default to "standard" varieties of English; based on evaluation by native speakers, we also find that model responses to non-"standard" varieties consistently exhibit a range of issues: stereotyping (19\% worse than for "standard" varieties), demeaning content (25\% worse), lack of comprehension (9\% worse), and condescending responses (15\% worse). We also find that if these models are asked to imitate the writing style of prompts in non-"standard" varieties, they produce text that exhibits lower comprehension of the input and is especially prone to stereotyping. GPT-4 improves on GPT-3.5 in terms of comprehension, warmth, and friendliness, but also exhibits a marked increase in stereotyping (+18\%). The results indicate that GPT-3.5 Turbo and GPT-4 can perpetuate linguistic discrimination toward speakers of non-"standard" varieties.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Fleisig, Eve and Smith, Genevieve and Bossi, Madeline and Rustagi, Ishita and Yin, Xavier and Klein, Dan},
	year = {2024},
	note = {Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Computers and Society (cs.CY)},
	file = {Fleisig et al. - 2024 - Linguistic Bias in ChatGPT Language Models Reinfo.pdf:/Users/daniel/Zotero/storage/SBCPE7VX/Fleisig et al. - 2024 - Linguistic Bias in ChatGPT Language Models Reinfo.pdf:application/pdf},
}

@INPROCEEDINGS{10516659,
  author={Birhane, Abeba and Steed, Ryan and Ojewale, Victor and Vecchione, Briana and Raji, Inioluwa Deborah},
  booktitle={2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)}, 
  title={AI auditing: The Broken Bus on the Road to AI Accountability}, 
  year={2024},
  volume={},
  number={},
  pages={612-643},
  keywords={Regulators;Roads;Design methodology;Ecosystems;Machine learning;Journalism;Regulation;Index Terms—Evaluation;auditing;accountability;transparency;artificial intelligence;society;law;machine learning;data science},
  doi={10.1109/SaTML59370.2024.00037}}

@article{VELDHUIS2025100708,
title = {Critical Artificial Intelligence literacy: A scoping review and framework synthesis},
journal = {International Journal of Child-Computer Interaction},
volume = {43},
pages = {100708},
year = {2025},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100708},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000771},
author = {Annemiek Veldhuis and Priscilla Y. Lo and Sadhbh Kenny and Alissa N. Antle},
keywords = {Artificial intelligence, Critical literacy, AI ethics, AI literacy, Computational empowerment, Literature review},
abstract = {The proliferation of Artificial Intelligence (AI) in everyday life raises concerns for children, other marginalized groups, and the general public. As new AI implementations continue to emerge, it is crucial to enable children to engage critically with AI. Critical literacy objectives and practices can encourage children to question, critique, and transform the social, political, cultural, and ethical implications of AI. As an initial step towards critical AI education, we conducted a 10-year scoping review to identify publications reporting on activities that engage children, between the ages of 5 and 18, to address the critical implications of AI. Our review identifies a wide range of participants, content, and pedagogical approaches. Through framework synthesis guided by an established critical literacy model, we examine the critical literacy learning objectives embedded in the reported activities and propose a critical AI literacy framework. This paper outlines future opportunities for critical AI literacies in the field of child–computer interaction including inspiring new learning activities, encouraging inclusive perspectives, and supporting pragmatic curriculum integration.}
}


@techreport{mcginnity_measures_2021,
	title = {Measures to combat racial discrimination and promote diversity in the labour market: a review of evidence},
	shorttitle = {Measures to combat racial discrimination and promote diversity in the labour market},
	url = {https://www.esri.ie/publications/measures-to-combat-racial-discrimination-and-promote-diversity-in-the-labour-market-a},
	abstract = {Racial discrimination in this report is understood to mean ‘any distinction, exclusion, restriction or preference based on race, colour, descent, or national or ethnic origin’ (ICERD, Article 1). Discrimination is distinct from racial prejudice (an attitude) and stereotypes (beliefs). Discrimination can be damaging to both individuals’ life chances and their wellbeing, as well as to society (OECD, 2013; Fibbi et al., 2021). Yet discrimination is difficult to measure accurately. It is also challenging to devise measures to combat discriminatory behaviour and promote diversity. This report reviews international literature on racial discrimination in the labour market and the effectiveness of measures to combat it. The aim is to distil the evidence into a short report to inform measures addressing discrimination in the labour market, including the current development of the National Action Plan Against Racism. The focus is on specific measures that can be implemented now to address current racial discrimination in the labour market.},
	urldate = {2025-02-04},
	institution = {ESRI},
	author = {McGinnity, Frances and Quinn, Emma and McCullough, Evie and Enright, Shannen and Curristan, Sarah},
	month = dec,
	year = {2021},
	doi = {10.26504/sustat110},
	file = {Full Text:C\:\\Users\\daniel.capistrano\\Zotero\\storage\\WHCTJ758\\ESRI et al. - 2021 - Measures to combat racial discrimination and promo.pdf:application/pdf},
}
